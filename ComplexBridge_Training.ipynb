{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TipsyPanda/ComplexBridges/blob/main/ComplexBridge_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80c5ea2d",
      "metadata": {
        "id": "80c5ea2d"
      },
      "source": [
        "# ComplexBridge – Feature Consumer & Model Trainer\n",
        "*Generated on 2025-10-15 16:40:07*\n",
        "\n",
        "This notebook **consumes precomputed features from `ARTIFACT_DIR`** and trains per-**span × sensor_type** models:\n",
        "\n",
        "- **Strain gauge** → Isolation Forest (tabular window features)  \n",
        "- **Accelerometer** → 1D CNN Autoencoder on **feature sequences**  \n",
        "- **Temperature** → SARIMAX (uses raw temperature streams from `DATA_PATH`)\n",
        "\n",
        "> Expected feature files in `ARTIFACT_DIR`: `feats_strain.(parquet|csv)`, `feats_accel.(parquet|csv)`, `feats_temp.(parquet|csv)`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7212052b",
      "metadata": {
        "id": "7212052b"
      },
      "source": [
        "## 1) Setup & Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1e010697",
      "metadata": {
        "id": "1e010697",
        "outputId": "0fa5419e-b055-4763-e2c7-58222df39988",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "DATA_PATH   : /content/drive/MyDrive/ComplexBridge_work/Data/\n",
            "ARTIFACT_DIR: /content/drive/MyDrive/ComplexBridge_work/Artifacts/\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# %pip install pandas numpy scikit-learn tensorflow statsmodels matplotlib pyarrow fastparquet\n",
        "\n",
        "import os, json, joblib, numpy as np, pandas as pd\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import average_precision_score, precision_recall_fscore_support\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "# Paths\n",
        "DATA_PATH = \"/content/drive/MyDrive/ComplexBridge_work/Data/\"\n",
        "ARTIFACT_DIR = \"/content/drive/MyDrive/ComplexBridge_work/Artifacts/\"\n",
        "\n",
        "print(\"DATA_PATH   :\", DATA_PATH)\n",
        "print(\"ARTIFACT_DIR:\", ARTIFACT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "283d9a79",
      "metadata": {
        "id": "283d9a79"
      },
      "source": [
        "## 2) Feature Dictionaries & Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d0a5c487",
      "metadata": {
        "id": "d0a5c487"
      },
      "outputs": [],
      "source": [
        "\n",
        "feature_dict = {\n",
        "    \"strain_gauge\":      [\"f_mean\",\"f_std\",\"f_rms\",\"f_p2p\",\"f_slope\",\"fft_low\",\"fft_mid\",\"fft_high\"],\n",
        "    \"accelerometer_rms\": [\"f_rms\",\"fft_centroid\",\"fft_entropy\",\"fft_dominant\"],\n",
        "    \"temperature\":       [\"f_mean\",\"f_slope\",\"ctx_tl_mean\"],\n",
        "}\n",
        "\n",
        "SEQ_LEN = 30  # number of feature-rows per CNN sequence\n",
        "\n",
        "def save_json(path, obj):\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(obj, f, indent=2)\n",
        "\n",
        "def model_path(span_id, sensor_type, name):\n",
        "    d = os.path.join(ARTIFACT_DIR, f\"{span_id}_{sensor_type}\")\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "    return os.path.join(d, name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a711058",
      "metadata": {
        "id": "3a711058"
      },
      "source": [
        "## 3) Load Precomputed Feature Matrices from `ARTIFACT_DIR`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e287e132",
      "metadata": {
        "id": "e287e132",
        "outputId": "32ff54ba-59eb-4373-b68d-91a1783a553e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "strain → shape=(1198, 13), cols=['t_center', 'sensor_id', 'sensor_type', 'span_id', 'f_mean', 'f_std', 'f_rms', 'f_p2p', 'f_slope', 'fft_low', 'fft_mid', 'fft_high']\n",
            "accel  → shape=(1198, 9), cols=['t_center', 'sensor_id', 'sensor_type', 'span_id', 'f_rms', 'fft_centroid', 'fft_entropy', 'fft_dominant', 'label_rule']\n",
            "temp   → shape=(10, 8), cols=['t_center', 'sensor_id', 'sensor_type', 'span_id', 'f_mean', 'f_slope', 'ctx_tl_mean', 'label_rule']\n"
          ]
        }
      ],
      "source": [
        "# Prefer scaled → fall back to unscaled; prefer parquet → fall back to csv\n",
        "def _try_read_any(bases):\n",
        "    for base in bases:\n",
        "        pq, cs = base + \".parquet\", base + \".csv\"\n",
        "        if os.path.exists(pq):\n",
        "            return pd.read_parquet(pq)\n",
        "        if os.path.exists(cs):\n",
        "            # parse t_center if present\n",
        "            return pd.read_csv(cs, parse_dates=['t_center'] if 't_center' in pd.read_csv(cs, nrows=0).columns else None)\n",
        "    return pd.DataFrame()\n",
        "\n",
        "def read_feats(sensor_type: str, use_scaled: bool = True) -> pd.DataFrame:\n",
        "    bases = []\n",
        "    if use_scaled:\n",
        "        bases.append(os.path.join(ARTIFACT_DIR, f\"features/feats_{sensor_type}_scaled\"))\n",
        "    bases.append(os.path.join(ARTIFACT_DIR, f\"features/feats_{sensor_type}\"))\n",
        "    df = _try_read_any(bases)\n",
        "    # ensure datetime\n",
        "    if not df.empty and 't_center' in df.columns and not np.issubdtype(df['t_center'].dtype, np.datetime64):\n",
        "        df['t_center'] = pd.to_datetime(df['t_center'], errors='coerce')\n",
        "    return df\n",
        "\n",
        "# Load latest features per new naming convention\n",
        "feats_strain = read_feats(\"strain_gauge\",      use_scaled=True)\n",
        "feats_accel  = read_feats(\"accelerometer_rms\", use_scaled=True)\n",
        "feats_temp   = read_feats(\"temperature\",       use_scaled=True)\n",
        "\n",
        "for name, F in [(\"strain\", feats_strain), (\"accel\", feats_accel), (\"temp\", feats_temp)]:\n",
        "    cols_preview = list(F.columns)[:12] if not F.empty else []\n",
        "    print(f\"{name:6s} → shape={F.shape}, cols={cols_preview}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "152cfbad",
      "metadata": {
        "id": "152cfbad"
      },
      "source": [
        "## 5) Train per-span Models – **Strain gauge → Isolation Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f0c4675d",
      "metadata": {
        "id": "f0c4675d",
        "outputId": "d7c592f9-9134-411f-a990-dea3368e2833",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SPAN_1][strain_gauge] trained rows=599, thr=-0.12971\n",
            "[SPAN_2][strain_gauge] trained rows=599, thr=-0.14317\n"
          ]
        }
      ],
      "source": [
        "def train_isoforest_span(span_id: str, F: pd.DataFrame):\n",
        "    s_type = \"strain_gauge\"\n",
        "    if F.empty:\n",
        "        print(f\"[{span_id}][{s_type}] no features\")\n",
        "        return None\n",
        "\n",
        "    keep = ['t_center','sensor_id','sensor_type','span_id'] + feature_dict[s_type]\n",
        "    if 'label_rule' in F.columns: keep += ['label_rule']\n",
        "    keep = [c for c in keep if c in F.columns]\n",
        "    G = F[(F['span_id']==span_id) & (F['sensor_type']==s_type)][keep].sort_values('t_center').reset_index(drop=True)\n",
        "    if G.empty:\n",
        "        print(f\"[{span_id}][{s_type}] no rows after filtering\")\n",
        "        return None\n",
        "\n",
        "    feat_cols = feature_dict[s_type]\n",
        "    Xs = G[feat_cols].to_numpy()  # already scaled\n",
        "\n",
        "\n",
        "    iso = IsolationForest(n_estimators=300, contamination=\"auto\", random_state=42, n_jobs=-1)\n",
        "    iso.fit(Xs)\n",
        "    scores = iso.decision_function(Xs)\n",
        "    thr = float(np.quantile(scores, 0.005))\n",
        "\n",
        "    joblib.dump(iso, model_path(span_id, s_type, \"isoforest.pkl\"))\n",
        "    save_json( model_path(span_id, s_type, \"threshold.json\"),\n",
        "               {\"score_threshold\": thr, \"note\": \"score<thr => alert\"} )\n",
        "    print(f\"[{span_id}][{s_type}] trained rows={len(G)}, thr={thr:.5f}\")\n",
        "    return {\"scores\": scores, \"thr\": thr}\n",
        "\n",
        "if not feats_strain.empty:\n",
        "    spans = feats_strain['span_id'].dropna().unique().tolist() if 'span_id' in feats_strain.columns else []\n",
        "    for span in spans:\n",
        "        try:\n",
        "            train_isoforest_span(span, feats_strain)\n",
        "        except Exception as e:\n",
        "            print(f\"[{span}][strain_gauge] ERROR:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f5c2c73",
      "metadata": {
        "id": "2f5c2c73"
      },
      "source": [
        "## 6) Train per-span Models – **Accelerometer → 1D CNN Autoencoder (feature sequences)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "21f3ddac",
      "metadata": {
        "id": "21f3ddac",
        "outputId": "51127fef-839d-4a3a-8c25-481e80993ed3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 144ms/step - loss: 1.0581 - val_loss: 0.4669\n",
            "Epoch 2/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.7674 - val_loss: 0.3971\n",
            "Epoch 3/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.6378 - val_loss: 0.3915\n",
            "Epoch 4/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.6134 - val_loss: 0.3735\n",
            "Epoch 5/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.5544 - val_loss: 0.3477\n",
            "Epoch 6/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.5202 - val_loss: 0.3236\n",
            "Epoch 7/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.5234 - val_loss: 0.3064\n",
            "Epoch 8/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.4900 - val_loss: 0.2961\n",
            "Epoch 9/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.4442 - val_loss: 0.2857\n",
            "Epoch 10/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.4702 - val_loss: 0.2780\n",
            "Epoch 11/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.4363 - val_loss: 0.2687\n",
            "Epoch 12/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.4375 - val_loss: 0.2630\n",
            "Epoch 13/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.4285 - val_loss: 0.2599\n",
            "Epoch 14/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.4017 - val_loss: 0.2553\n",
            "Epoch 15/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.3815 - val_loss: 0.2502\n",
            "Epoch 16/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.3809 - val_loss: 0.2467\n",
            "Epoch 17/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.3877 - val_loss: 0.2430\n",
            "Epoch 18/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.3537 - val_loss: 0.2392\n",
            "Epoch 19/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.3522 - val_loss: 0.2373\n",
            "Epoch 20/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.3438 - val_loss: 0.2335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SPAN_1][accelerometer_rms] trained seq=569, thr=2.295877\n",
            "Epoch 1/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 142ms/step - loss: 0.8892 - val_loss: 0.4503\n",
            "Epoch 2/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.6077 - val_loss: 0.3911\n",
            "Epoch 3/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.5324 - val_loss: 0.3876\n",
            "Epoch 4/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.4871 - val_loss: 0.3613\n",
            "Epoch 5/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.4578 - val_loss: 0.3384\n",
            "Epoch 6/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.4292 - val_loss: 0.3122\n",
            "Epoch 7/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.3875 - val_loss: 0.2977\n",
            "Epoch 8/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.3765 - val_loss: 0.2883\n",
            "Epoch 9/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.3705 - val_loss: 0.2809\n",
            "Epoch 10/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.3545 - val_loss: 0.2762\n",
            "Epoch 11/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.3576 - val_loss: 0.2695\n",
            "Epoch 12/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.3248 - val_loss: 0.2673\n",
            "Epoch 13/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.3069 - val_loss: 0.2646\n",
            "Epoch 14/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.3003 - val_loss: 0.2606\n",
            "Epoch 15/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.2966 - val_loss: 0.2567\n",
            "Epoch 16/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.2812 - val_loss: 0.2539\n",
            "Epoch 17/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.2919 - val_loss: 0.2499\n",
            "Epoch 18/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.2715 - val_loss: 0.2472\n",
            "Epoch 19/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.2726 - val_loss: 0.2438\n",
            "Epoch 20/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.2650 - val_loss: 0.2405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SPAN_2][accelerometer_rms] trained seq=569, thr=1.176330\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def build_feature_sequences(G: pd.DataFrame, seq_len: int = SEQ_LEN):\n",
        "    # Build sequences from feature rows per sensor, then stack for span-level model.\n",
        "    feat_cols = feature_dict[\"accelerometer_rms\"]\n",
        "    X_seq = []\n",
        "    for sid, g in G.groupby('sensor_id'):\n",
        "        g = g.sort_values('t_center')\n",
        "        X = g[feat_cols].to_numpy().astype('float32')\n",
        "        for i in range(len(X) - seq_len):\n",
        "            X_seq.append(X[i:i+seq_len])\n",
        "    if not X_seq:\n",
        "        return None\n",
        "    X_seq = np.stack(X_seq)  # (n_seq, seq_len, feat_dim)\n",
        "    return X_seq\n",
        "\n",
        "def build_cnn_ae(seq_len: int, feat_dim: int, latent: int = 32):\n",
        "    inp = layers.Input(shape=(seq_len, feat_dim))\n",
        "    # Encoder\n",
        "    x = layers.Conv1D(64, 5, padding=\"same\", activation=\"relu\")(inp)\n",
        "    x = layers.MaxPool1D(2, padding=\"same\")(x)        # 30 -> 15\n",
        "    x = layers.Conv1D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.MaxPool1D(2, padding=\"same\")(x)        # 15 -> 8 (ceil)\n",
        "    x = layers.Conv1D(latent, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "\n",
        "    # Decoder\n",
        "    x = layers.UpSampling1D(2)(x)                     # 8 -> 16\n",
        "    x = layers.Conv1D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.UpSampling1D(2)(x)                     # 16 -> 32\n",
        "    x = layers.Conv1D(feat_dim, 3, padding=\"same\")(x) # 32\n",
        "\n",
        "    # Crop back to the original seq_len to guarantee shape match\n",
        "    out = layers.Lambda(lambda t: t[:, :seq_len, :])(x)  # 32 -> 30\n",
        "\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_cnn_span(span_id: str, F: pd.DataFrame):\n",
        "    s_type = \"accelerometer_rms\"\n",
        "    if F.empty:\n",
        "        print(f\"[{span_id}][{s_type}] no features\")\n",
        "        return None\n",
        "\n",
        "    keep = ['t_center','sensor_id','sensor_type','span_id'] + feature_dict[s_type]\n",
        "    if 'label_rule' in F.columns: keep += ['label_rule']\n",
        "    keep = [c for c in keep if c in F.columns]\n",
        "    G = F[(F['span_id']==span_id) & (F['sensor_type']==s_type)][keep].sort_values('t_center')\n",
        "    if G.empty:\n",
        "        print(f\"[{span_id}][{s_type}] no rows after filtering\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    feat_cols = feature_dict[s_type]\n",
        "    G[feat_cols].to_numpy()  # already scaled\n",
        "\n",
        "\n",
        "    X_seq = build_feature_sequences(G, seq_len=SEQ_LEN)\n",
        "    if X_seq is None:\n",
        "        print(f\"[{span_id}][{s_type}] insufficient sequences\")\n",
        "        return None\n",
        "\n",
        "    n = len(X_seq)\n",
        "    n_tr = max(int(n*0.8), 1)\n",
        "    Xtr, Xva = X_seq[:n_tr], X_seq[n_tr:] if n_tr < n else (X_seq, X_seq[:0])\n",
        "\n",
        "    model = build_cnn_ae(seq_len=SEQ_LEN, feat_dim=X_seq.shape[2], latent=32)\n",
        "    model.fit(Xtr, Xtr, validation_data=(Xva, Xva) if len(Xva)>0 else None, epochs=20, batch_size=128, verbose=1)\n",
        "\n",
        "    recon = model.predict(X_seq, verbose=0)\n",
        "    err = np.mean((X_seq - recon)**2, axis=(1,2))\n",
        "    thr = float(np.quantile(err, 0.995))\n",
        "\n",
        "    model.save(model_path(span_id, s_type, \"cnn_ae_features.h5\"))\n",
        "    save_json(model_path(span_id, s_type, \"threshold.json\"),\n",
        "              {\"recon_err_threshold\": thr, \"note\": \"err>thr => alert (on feature sequences)\"})\n",
        "    print(f\"[{span_id}][{s_type}] trained seq={len(X_seq)}, thr={thr:.6f}\")\n",
        "    return {\"thr\": thr, \"n_seq\": len(X_seq)}\n",
        "\n",
        "if not feats_accel.empty:\n",
        "    spans = feats_accel['span_id'].dropna().unique().tolist() if 'span_id' in feats_accel.columns else []\n",
        "    for span in spans:\n",
        "        try:\n",
        "            train_cnn_span(span, feats_accel)\n",
        "        except Exception as e:\n",
        "            print(f\"[{span}][accelerometer_rms] ERROR:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dff4d49",
      "metadata": {
        "id": "5dff4d49"
      },
      "source": [
        "## 7) Train per-span Models – **Temperature → SARIMAX (raw streams)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0f450f77",
      "metadata": {
        "id": "0f450f77"
      },
      "outputs": [],
      "source": [
        "\n",
        "def fit_best_sarimax(y, orders=[(1,0,0),(1,1,0),(2,0,1),(2,1,1)]):\n",
        "    best = None\n",
        "    for (p,d,q) in orders:\n",
        "        try:\n",
        "            m = sm.tsa.SARIMAX(y, order=(p,d,q), enforce_stationarity=False, enforce_invertibility=False)\n",
        "            r = m.fit(disp=False)\n",
        "            if (best is None) or (r.aic < best[0]):\n",
        "                best = (r.aic, r)\n",
        "        except Exception:\n",
        "            continue\n",
        "    return best[1] if best else None\n",
        "\n",
        "# Optional: enable temperature training if you provide DATA_PATH/raw.csv with temperature streams\n",
        "ENABLE_TEMP = False\n",
        "\n",
        "def train_temp_span_raw(span_id: str):\n",
        "    raw_csv = os.path.join(DATA_PATH, \"raw.csv\")\n",
        "    if not os.path.exists(raw_csv):\n",
        "        print(f\"[{span_id}][temperature] raw.csv not found -> skipping\")\n",
        "        return None\n",
        "    df_raw = pd.read_csv(raw_csv, parse_dates=['timestamp'])\n",
        "    df_raw = df_raw.sort_values('timestamp').set_index('timestamp')\n",
        "    df_span = df_raw[(df_raw['span_id']==span_id) & (df_raw['sensor_type']==\"temperature\")].copy()\n",
        "    if df_span.empty:\n",
        "        print(f\"[{span_id}][temperature] no raw rows\")\n",
        "        return None\n",
        "\n",
        "    out = {}\n",
        "    for sid, g in df_span.groupby('sensor_id'):\n",
        "        y = g['value'].asfreq('1min').interpolate()\n",
        "        model = fit_best_sarimax(y)\n",
        "        if model is None:\n",
        "            continue\n",
        "        model.save(model_path(span_id, \"temperature\", f\"sarimax_{sid}.pkl\"))\n",
        "        out[sid] = {\"aic\": float(model.aic), \"n\": int(len(y))}\n",
        "        print(f\"[{span_id}][temperature][{sid}] AIC={model.aic:.1f}, n={len(y)}\")\n",
        "    save_json(model_path(span_id, \"temperature\", \"index.json\"), out)\n",
        "    return out\n",
        "\n",
        "if ENABLE_TEMP:\n",
        "    spans = set()\n",
        "    if 'span_id' in feats_temp.columns: spans.update(feats_temp['span_id'].dropna().unique().tolist())\n",
        "    if 'span_id' in feats_strain.columns: spans.update(feats_strain['span_id'].dropna().unique().tolist())\n",
        "    if 'span_id' in feats_accel.columns: spans.update(feats_accel['span_id'].dropna().unique().tolist())\n",
        "    for span in spans:\n",
        "        try:\n",
        "            train_temp_span_raw(span)\n",
        "        except Exception as e:\n",
        "            print(f\"[{span}][temperature] ERROR:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cae39294",
      "metadata": {
        "id": "cae39294"
      },
      "source": [
        "## 8) Summary & Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1499480",
      "metadata": {
        "id": "b1499480"
      },
      "source": [
        "\n",
        "- Loads **precomputed features** from `ARTIFACT_DIR` and trains **per-span** models:\n",
        "  - Strain → Isolation Forest\n",
        "  - Accelerometer → 1D CNN Autoencoder (feature sequences)\n",
        "  - Temperature → SARIMAX (optional; requires raw temperature in `DATA_PATH/raw.csv`)\n",
        "- Saves artifacts under: `ARTIFACT_DIR/{SPAN}_{sensor_type}/...`\n",
        "\n",
        "**Next:** scoring notebook that loads these artifacts and produces anomaly scores for new windows.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}