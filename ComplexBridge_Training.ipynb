{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c5ea2d",
   "metadata": {},
   "source": [
    "# ComplexBridge – Feature Consumer & Model Trainer\n",
    "*Generated on 2025-10-15 16:40:07*\n",
    "\n",
    "This notebook **consumes precomputed features from `ARTIFACT_DIR`** and trains per-**span × sensor_type** models:\n",
    "\n",
    "- **Strain gauge** → Isolation Forest (tabular window features)  \n",
    "- **Accelerometer** → 1D CNN Autoencoder on **feature sequences**  \n",
    "- **Temperature** → SARIMAX (uses raw temperature streams from `DATA_PATH`)\n",
    "\n",
    "> Expected feature files in `ARTIFACT_DIR`: `feats_strain.(parquet|csv)`, `feats_accel.(parquet|csv)`, `feats_temp.(parquet|csv)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7212052b",
   "metadata": {},
   "source": [
    "## 1) Setup & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e010697",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %pip install pandas numpy scikit-learn tensorflow statsmodels matplotlib pyarrow fastparquet\n",
    "\n",
    "import os, json, joblib, numpy as np, pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import average_precision_score, precision_recall_fscore_support\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Paths (user-specified)\n",
    "DATA_PATH = \"/content/drive/MyDrive/ComplexBridge_work/Data/\"\n",
    "ARTIFACT_DIR = \"/content/drive/MyDrive/ComplexBridge_work/Artifacts/\"\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"DATA_PATH   :\", DATA_PATH)\n",
    "print(\"ARTIFACT_DIR:\", ARTIFACT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283d9a79",
   "metadata": {},
   "source": [
    "## 2) Feature Dictionaries & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_dict = { \n",
    "    \"strain_gauge\":      [\"f_mean\",\"f_std\",\"f_rms\",\"f_p2p\",\"f_slope\",\"fft_low\",\"fft_mid\",\"fft_high\"],\n",
    "    \"accelerometer_rms\": [\"f_rms\",\"fft_centroid\",\"fft_entropy\",\"fft_dominant\"],\n",
    "    \"temperature\":       [\"f_mean\",\"f_slope\",\"ctx_tl_mean\"],\n",
    "}\n",
    "\n",
    "SEQ_LEN = 30  # number of feature-rows per CNN sequence\n",
    "\n",
    "def save_json(path, obj):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "def model_path(span_id, sensor_type, name):\n",
    "    d = os.path.join(ARTIFACT_DIR, f\"{span_id}_{sensor_type}\")\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "    return os.path.join(d, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a711058",
   "metadata": {},
   "source": [
    "## 3) Load Precomputed Feature Matrices from `ARTIFACT_DIR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _try_read(base):\n",
    "    pq = base + \".parquet\"\n",
    "    cs = base + \".csv\"\n",
    "    if os.path.exists(pq):\n",
    "        return pd.read_parquet(pq)\n",
    "    if os.path.exists(cs):\n",
    "        return pd.read_csv(cs, parse_dates=['t_center'])\n",
    "    return pd.DataFrame()\n",
    "\n",
    "feats_strain = _try_read(os.path.join(ARTIFACT_DIR, \"feats_strain\"))\n",
    "feats_accel  = _try_read(os.path.join(ARTIFACT_DIR, \"feats_accel\"))\n",
    "feats_temp   = _try_read(os.path.join(ARTIFACT_DIR, \"feats_temp\"))\n",
    "\n",
    "for name, F in [(\"strain\", feats_strain), (\"accel\", feats_accel), (\"temp\", feats_temp)]:\n",
    "    print(name, F.shape, list(F.columns)[:12])\n",
    "\n",
    "# ensure datetime\n",
    "for F in [feats_strain, feats_accel, feats_temp]:\n",
    "    if not F.empty and not np.issubdtype(F['t_center'].dtype, np.datetime64):\n",
    "        F['t_center'] = pd.to_datetime(F['t_center'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd414af2",
   "metadata": {},
   "source": [
    "## 4) Load or Fit Per-Type Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e1479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scalers: Dict[str, RobustScaler] = {}\n",
    "\n",
    "def load_or_fit_scaler(sensor_type: str, F: pd.DataFrame) -> RobustScaler:\n",
    "    path = os.path.join(ARTIFACT_DIR, f\"scaler_{sensor_type}.joblib\")\n",
    "    feat_cols = [c for c in F.columns if c not in ('t_center','sensor_id','sensor_type','span_id','label_rule')]\n",
    "    if os.path.exists(path):\n",
    "        sc = joblib.load(path)\n",
    "    else:\n",
    "        sc = RobustScaler().fit(F[feat_cols])\n",
    "        joblib.dump(sc, path)\n",
    "    return sc\n",
    "\n",
    "if not feats_strain.empty:\n",
    "    scalers[\"strain_gauge\"] = load_or_fit_scaler(\"strain_gauge\", feats_strain)\n",
    "if not feats_accel.empty:\n",
    "    scalers[\"accelerometer_rms\"] = load_or_fit_scaler(\"accelerometer_rms\", feats_accel)\n",
    "if not feats_temp.empty:\n",
    "    scalers[\"temperature\"] = load_or_fit_scaler(\"temperature\", feats_temp)\n",
    "\n",
    "print(\"Loaded scalers for:\", list(scalers.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152cfbad",
   "metadata": {},
   "source": [
    "## 5) Train per-span Models – **Strain gauge → Isolation Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c4675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def train_isoforest_span(span_id: str, F: pd.DataFrame):\n",
    "    s_type = \"strain_gauge\"\n",
    "    if F.empty: \n",
    "        print(f\"[{span_id}][{s_type}] no features\")\n",
    "        return None\n",
    "    \n",
    "    keep = ['t_center','sensor_id','sensor_type','span_id'] + feature_dict[s_type]\n",
    "    if 'label_rule' in F.columns: keep += ['label_rule']\n",
    "    keep = [c for c in keep if c in F.columns]\n",
    "    G = F[(F['span_id']==span_id) & (F['sensor_type']==s_type)][keep].sort_values('t_center').reset_index(drop=True)\n",
    "    if G.empty:\n",
    "        print(f\"[{span_id}][{s_type}] no rows after filtering\")\n",
    "        return None\n",
    "\n",
    "    feat_cols = feature_dict[s_type]\n",
    "    sc = scalers[s_type]\n",
    "    Xs = sc.transform(G[feat_cols])\n",
    "\n",
    "    iso = IsolationForest(n_estimators=300, contamination=\"auto\", random_state=42, n_jobs=-1)\n",
    "    iso.fit(Xs)\n",
    "    scores = iso.decision_function(Xs)\n",
    "    thr = float(np.quantile(scores, 0.005))\n",
    "\n",
    "    joblib.dump(iso, model_path(span_id, s_type, \"isoforest.pkl\"))\n",
    "    save_json( model_path(span_id, s_type, \"threshold.json\"),\n",
    "               {\"score_threshold\": thr, \"note\": \"score<thr => alert\"} )\n",
    "    print(f\"[{span_id}][{s_type}] trained rows={len(G)}, thr={thr:.5f}\")\n",
    "    return {\"scores\": scores, \"thr\": thr}\n",
    "\n",
    "if not feats_strain.empty:\n",
    "    spans = feats_strain['span_id'].dropna().unique().tolist() if 'span_id' in feats_strain.columns else []\n",
    "    for span in spans:\n",
    "        try:\n",
    "            train_isoforest_span(span, feats_strain)\n",
    "        except Exception as e:\n",
    "            print(f\"[{span}][strain_gauge] ERROR:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c2c73",
   "metadata": {},
   "source": [
    "## 6) Train per-span Models – **Accelerometer → 1D CNN Autoencoder (feature sequences)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f3ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_feature_sequences(G: pd.DataFrame, seq_len: int = SEQ_LEN):\n",
    "    # Build sequences from feature rows per sensor, then stack for span-level model.\n",
    "    feat_cols = feature_dict[\"accelerometer_rms\"]\n",
    "    X_seq = []\n",
    "    for sid, g in G.groupby('sensor_id'):\n",
    "        g = g.sort_values('t_center')\n",
    "        X = g[feat_cols].to_numpy().astype('float32')\n",
    "        for i in range(len(X) - seq_len):\n",
    "            X_seq.append(X[i:i+seq_len])\n",
    "    if not X_seq:\n",
    "        return None\n",
    "    X_seq = np.stack(X_seq)  # (n_seq, seq_len, feat_dim)\n",
    "    return X_seq\n",
    "\n",
    "def build_cnn_ae(seq_len: int, feat_dim: int, latent: int = 32):\n",
    "    inp = layers.Input(shape=(seq_len, feat_dim))\n",
    "    x = layers.Conv1D(64, 5, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.MaxPool1D(2)(x)\n",
    "    x = layers.Conv1D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPool1D(2)(x)\n",
    "    x = layers.Conv1D(latent, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.UpSampling1D(2)(x)\n",
    "    x = layers.Conv1D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.UpSampling1D(2)(x)\n",
    "    out = layers.Conv1D(feat_dim, 3, padding=\"same\")(x)\n",
    "    model = models.Model(inp, out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def train_cnn_span(span_id: str, F: pd.DataFrame):\n",
    "    s_type = \"accelerometer_rms\"\n",
    "    if F.empty:\n",
    "        print(f\"[{span_id}][{s_type}] no features\")\n",
    "        return None\n",
    "\n",
    "    keep = ['t_center','sensor_id','sensor_type','span_id'] + feature_dict[s_type]\n",
    "    if 'label_rule' in F.columns: keep += ['label_rule']\n",
    "    keep = [c for c in keep if c in F.columns]\n",
    "    G = F[(F['span_id']==span_id) & (F['sensor_type']==s_type)][keep].sort_values('t_center')\n",
    "    if G.empty:\n",
    "        print(f\"[{span_id}][{s_type}] no rows after filtering\")\n",
    "        return None\n",
    "\n",
    "    sc = scalers[s_type]\n",
    "    feat_cols = feature_dict[s_type]\n",
    "    G[feat_cols] = sc.transform(G[feat_cols])\n",
    "\n",
    "    X_seq = build_feature_sequences(G, seq_len=SEQ_LEN)\n",
    "    if X_seq is None:\n",
    "        print(f\"[{span_id}][{s_type}] insufficient sequences\")\n",
    "        return None\n",
    "\n",
    "    n = len(X_seq)\n",
    "    n_tr = max(int(n*0.8), 1)\n",
    "    Xtr, Xva = X_seq[:n_tr], X_seq[n_tr:] if n_tr < n else (X_seq, X_seq[:0])\n",
    "\n",
    "    model = build_cnn_ae(seq_len=SEQ_LEN, feat_dim=X_seq.shape[2], latent=32)\n",
    "    model.fit(Xtr, Xtr, validation_data=(Xva, Xva) if len(Xva)>0 else None, epochs=20, batch_size=128, verbose=1)\n",
    "\n",
    "    recon = model.predict(X_seq, verbose=0)\n",
    "    err = np.mean((X_seq - recon)**2, axis=(1,2))\n",
    "    thr = float(np.quantile(err, 0.995))\n",
    "\n",
    "    model.save(model_path(span_id, s_type, \"cnn_ae_features.h5\"))\n",
    "    save_json(model_path(span_id, s_type, \"threshold.json\"),\n",
    "              {\"recon_err_threshold\": thr, \"note\": \"err>thr => alert (on feature sequences)\"})\n",
    "    print(f\"[{span_id}][{s_type}] trained seq={len(X_seq)}, thr={thr:.6f}\")\n",
    "    return {\"thr\": thr, \"n_seq\": len(X_seq)}\n",
    "\n",
    "if not feats_accel.empty:\n",
    "    spans = feats_accel['span_id'].dropna().unique().tolist() if 'span_id' in feats_accel.columns else []\n",
    "    for span in spans:\n",
    "        try:\n",
    "            train_cnn_span(span, feats_accel)\n",
    "        except Exception as e:\n",
    "            print(f\"[{span}][accelerometer_rms] ERROR:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dff4d49",
   "metadata": {},
   "source": [
    "## 7) Train per-span Models – **Temperature → SARIMAX (raw streams)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f450f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_best_sarimax(y, orders=[(1,0,0),(1,1,0),(2,0,1),(2,1,1)]):\n",
    "    best = None\n",
    "    for (p,d,q) in orders:\n",
    "        try:\n",
    "            m = sm.tsa.SARIMAX(y, order=(p,d,q), enforce_stationarity=False, enforce_invertibility=False)\n",
    "            r = m.fit(disp=False)\n",
    "            if (best is None) or (r.aic < best[0]):\n",
    "                best = (r.aic, r)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return best[1] if best else None\n",
    "\n",
    "# Optional: enable temperature training if you provide DATA_PATH/raw.csv with temperature streams\n",
    "ENABLE_TEMP = False\n",
    "\n",
    "def train_temp_span_raw(span_id: str):\n",
    "    raw_csv = os.path.join(DATA_PATH, \"raw.csv\")\n",
    "    if not os.path.exists(raw_csv):\n",
    "        print(f\"[{span_id}][temperature] raw.csv not found -> skipping\")\n",
    "        return None\n",
    "    df_raw = pd.read_csv(raw_csv, parse_dates=['timestamp'])\n",
    "    df_raw = df_raw.sort_values('timestamp').set_index('timestamp')\n",
    "    df_span = df_raw[(df_raw['span_id']==span_id) & (df_raw['sensor_type']==\"temperature\")].copy()\n",
    "    if df_span.empty:\n",
    "        print(f\"[{span_id}][temperature] no raw rows\")\n",
    "        return None\n",
    "\n",
    "    out = {}\n",
    "    for sid, g in df_span.groupby('sensor_id'):\n",
    "        y = g['value'].asfreq('1min').interpolate()\n",
    "        model = fit_best_sarimax(y)\n",
    "        if model is None: \n",
    "            continue\n",
    "        model.save(model_path(span_id, \"temperature\", f\"sarimax_{sid}.pkl\"))\n",
    "        out[sid] = {\"aic\": float(model.aic), \"n\": int(len(y))}\n",
    "        print(f\"[{span_id}][temperature][{sid}] AIC={model.aic:.1f}, n={len(y)}\")\n",
    "    save_json(model_path(span_id, \"temperature\", \"index.json\"), out)\n",
    "    return out\n",
    "\n",
    "if ENABLE_TEMP:\n",
    "    spans = set()\n",
    "    if 'span_id' in feats_temp.columns: spans.update(feats_temp['span_id'].dropna().unique().tolist())\n",
    "    if 'span_id' in feats_strain.columns: spans.update(feats_strain['span_id'].dropna().unique().tolist())\n",
    "    if 'span_id' in feats_accel.columns: spans.update(feats_accel['span_id'].dropna().unique().tolist())\n",
    "    for span in spans:\n",
    "        try:\n",
    "            train_temp_span_raw(span)\n",
    "        except Exception as e:\n",
    "            print(f\"[{span}][temperature] ERROR:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae39294",
   "metadata": {},
   "source": [
    "## 8) Summary & Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1499480",
   "metadata": {},
   "source": [
    "\n",
    "- Loads **precomputed features** from `ARTIFACT_DIR` and trains **per-span** models:\n",
    "  - Strain → Isolation Forest\n",
    "  - Accelerometer → 1D CNN Autoencoder (feature sequences)\n",
    "  - Temperature → SARIMAX (optional; requires raw temperature in `DATA_PATH/raw.csv`)\n",
    "- Saves artifacts under: `ARTIFACT_DIR/{SPAN}_{sensor_type}/...`\n",
    "\n",
    "**Next:** scoring notebook that loads these artifacts and produces anomaly scores for new windows.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
