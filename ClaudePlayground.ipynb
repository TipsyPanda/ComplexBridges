{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TipsyPanda/ComplexBridges/blob/main/ClaudePlayground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpAEYQEKUVAu"
      },
      "outputs": [],
      "source": [
        "# IPMB – Multi-Model Ensemble Anomaly Detection\n",
        "# Extended notebook with sensor-type-specific models\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML imports\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import precision_recall_fscore_support, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Deep learning imports (TensorFlow/Keras for LSTM Autoencoder & 1D CNN)\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers, Model\n",
        "    KERAS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    KERAS_AVAILABLE = False\n",
        "    print(\"Warning: TensorFlow not available. LSTM/CNN models will be skipped.\")\n",
        "\n",
        "# Time series imports for ARIMA\n",
        "try:\n",
        "    from statsmodels.tsa.arima.model import ARIMA\n",
        "    from scipy.signal import medfilt\n",
        "    STATSMODELS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    STATSMODELS_AVAILABLE = False\n",
        "    print(\"Warning: statsmodels not available. ARIMA models will be skipped.\")\n",
        "\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION\n",
        "# ==============================================================================\n",
        "DATA_PATH = \"/content/drive/MyDrive/ComplexBridge_work/Data/\"\n",
        "ARTIFACT_DIR = \"/content/drive/MyDrive/ComplexBridge_work/Artifacts/\"\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 4)"
      ],
      "metadata": {
        "id": "y8SzI07kUmh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# DATA LOADING\n",
        "# ==============================================================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "def load_df_with_datetimeindex(path: str, time_col: str = \"timestamp\"):\n",
        "    \"\"\"Load CSV/Pickle and ensure DatetimeIndex.\"\"\"\n",
        "    if path.lower().endswith((\".pkl\", \".pickle\")):\n",
        "        df = pd.read_pickle(path)\n",
        "    else:\n",
        "        df = pd.read_csv(path)\n",
        "\n",
        "    if isinstance(df.index, pd.DatetimeIndex):\n",
        "        return df.sort_index()\n",
        "\n",
        "    if time_col in df.columns:\n",
        "        df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n",
        "        df = df.dropna(subset=[time_col]).set_index(time_col).sort_index()\n",
        "        return df\n",
        "\n",
        "    try:\n",
        "        idx = pd.to_datetime(df.index, errors='raise')\n",
        "        df.index = idx\n",
        "        return df.sort_index()\n",
        "    except Exception:\n",
        "        raise ValueError(\"No DatetimeIndex found. Provide 'timestamp' column.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "3_pwHZ1xUq7H",
        "outputId": "6d66ed69-0065-4664-8b86-7b5262a105c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Mountpoint must not already contain files",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4054415477.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ==============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_df_with_datetimeindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_col\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# FEATURE ENGINEERING\n",
        "# ==============================================================================\n",
        "def infer_base_freq(idx: pd.DatetimeIndex) -> str:\n",
        "    \"\"\"Infer base frequency from DatetimeIndex.\"\"\"\n",
        "    guessed = pd.infer_freq(idx)\n",
        "    if guessed is not None:\n",
        "        return guessed\n",
        "    deltas = idx.to_series().diff().dropna().dt.total_seconds()\n",
        "    if len(deltas) == 0:\n",
        "        return \"1S\"\n",
        "    ms = int(np.median(deltas) * 1000)\n",
        "    ms = max(ms, 1)\n",
        "    return f\"{ms}L\"\n",
        "\n",
        "def make_rolling_features(ts: pd.DataFrame, window: str = '30s', step: str = '15s'):\n",
        "    \"\"\"Create rolling window features for traditional ML models.\"\"\"\n",
        "    if not isinstance(ts.index, pd.DatetimeIndex):\n",
        "        raise TypeError(\"Expected DatetimeIndex\")\n",
        "    ts = ts.sort_index()\n",
        "\n",
        "    base_freq = infer_base_freq(ts.index)\n",
        "    ts = ts.resample(base_freq).ffill()\n",
        "\n",
        "    start, end = ts.index.min(), ts.index.max()\n",
        "    out = []\n",
        "    cur = start\n",
        "    w = pd.to_timedelta(window)\n",
        "    s = pd.to_timedelta(step)\n",
        "\n",
        "    while cur + w <= end:\n",
        "        win = ts.loc[cur:cur+w]\n",
        "        if len(win) < 3:\n",
        "            cur += s\n",
        "            continue\n",
        "\n",
        "        v = win['value'].values\n",
        "        mean = float(np.mean(v))\n",
        "        std = float(np.std(v, ddof=1)) if len(v) > 1 else 0.0\n",
        "        rms = float(np.sqrt(np.mean(v**2)))\n",
        "        p2p = float(np.max(v) - np.min(v))\n",
        "        skew = float(pd.Series(v).skew()) if len(v) > 2 else 0.0\n",
        "        kurt = float(pd.Series(v).kurtosis()) if len(v) > 3 else 0.0\n",
        "        x = np.arange(len(v))\n",
        "        slope = float(np.polyfit(x, v, 1)[0]) if len(v) > 1 else 0.0\n",
        "        dv = np.diff(v)\n",
        "        adiff_mean = float(np.mean(np.abs(dv))) if len(dv) else 0.0\n",
        "\n",
        "        ctx = {}\n",
        "        if 'traffic_load_proxy' in win.columns:\n",
        "            tl = win['traffic_load_proxy'].values\n",
        "            ctx['ctx_tl_mean'] = float(np.mean(tl))\n",
        "            ctx['ctx_tl_std'] = float(np.std(tl, ddof=1)) if len(tl)>1 else 0.0\n",
        "        if 'rule_threshold' in win.columns:\n",
        "            ctx['ctx_rule_thr'] = float(np.median(win['rule_threshold']))\n",
        "\n",
        "        label = None\n",
        "        if 'anomaly' in win.columns:\n",
        "            label = int((win['anomaly'] == 1).any())\n",
        "\n",
        "        t_center = cur + w/2\n",
        "        row = {\n",
        "            't_center': t_center,\n",
        "            'f_mean': mean, 'f_std': std, 'f_rms': rms, 'f_p2p': p2p,\n",
        "            'f_skew': skew, 'f_kurt': kurt, 'f_slope': slope, 'f_adiff_mean': adiff_mean,\n",
        "            **ctx\n",
        "        }\n",
        "        if label is not None:\n",
        "            row['label_rule'] = label\n",
        "        out.append(row)\n",
        "        cur += s\n",
        "\n",
        "    return pd.DataFrame(out).sort_values('t_center').reset_index(drop=True)\n",
        "\n",
        "def create_sequences(data: np.ndarray, seq_len: int = 50):\n",
        "    \"\"\"Create sequences for LSTM/CNN models.\"\"\"\n",
        "    sequences = []\n",
        "    for i in range(len(data) - seq_len + 1):\n",
        "        sequences.append(data[i:i+seq_len])\n",
        "    return np.array(sequences)"
      ],
      "metadata": {
        "id": "3uqtUIrAUrgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# MODEL DEFINITIONS\n",
        "# ==============================================================================\n",
        "class LSTMAutoencoder:\n",
        "    \"\"\"LSTM Autoencoder for strain gauge and accelerometer data.\"\"\"\n",
        "    def __init__(self, seq_len: int = 50, latent_dim: int = 16):\n",
        "        self.seq_len = seq_len\n",
        "        self.latent_dim = latent_dim\n",
        "        self.model = None\n",
        "        self.scaler = RobustScaler()\n",
        "        self.threshold = None\n",
        "\n",
        "    def build_model(self):\n",
        "        if not KERAS_AVAILABLE:\n",
        "            raise RuntimeError(\"TensorFlow not available\")\n",
        "\n",
        "        # Encoder\n",
        "        encoder_input = layers.Input(shape=(self.seq_len, 1))\n",
        "        x = layers.LSTM(64, return_sequences=True)(encoder_input)\n",
        "        x = layers.LSTM(32, return_sequences=False)(x)\n",
        "        encoded = layers.Dense(self.latent_dim)(x)\n",
        "\n",
        "        # Decoder\n",
        "        x = layers.RepeatVector(self.seq_len)(encoded)\n",
        "        x = layers.LSTM(32, return_sequences=True)(x)\n",
        "        x = layers.LSTM(64, return_sequences=True)(x)\n",
        "        decoded = layers.TimeDistributed(layers.Dense(1))(x)\n",
        "\n",
        "        self.model = Model(encoder_input, decoded)\n",
        "        self.model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    def fit(self, data: np.ndarray, epochs: int = 50):\n",
        "        data_scaled = self.scaler.fit_transform(data.reshape(-1, 1)).flatten()\n",
        "        sequences = create_sequences(data_scaled, self.seq_len)\n",
        "        X = sequences.reshape(-1, self.seq_len, 1)\n",
        "\n",
        "        if self.model is None:\n",
        "            self.build_model()\n",
        "\n",
        "        self.model.fit(X, X, epochs=epochs, batch_size=32, verbose=0)\n",
        "\n",
        "        # Calculate reconstruction errors for threshold\n",
        "        recon = self.model.predict(X, verbose=0)\n",
        "        errors = np.mean(np.abs(X - recon), axis=(1, 2))\n",
        "        self.threshold = np.percentile(errors, 99.5)\n",
        "\n",
        "    def score(self, data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Return anomaly scores (higher = more anomalous).\"\"\"\n",
        "        data_scaled = self.scaler.transform(data.reshape(-1, 1)).flatten()\n",
        "        sequences = create_sequences(data_scaled, self.seq_len)\n",
        "        X = sequences.reshape(-1, self.seq_len, 1)\n",
        "\n",
        "        recon = self.model.predict(X, verbose=0)\n",
        "        errors = np.mean(np.abs(X - recon), axis=(1, 2))\n",
        "        return errors\n",
        "\n",
        "class CNN1D:\n",
        "    \"\"\"1D CNN for accelerometer oscillation patterns.\"\"\"\n",
        "    def __init__(self, seq_len: int = 100):\n",
        "        self.seq_len = seq_len\n",
        "        self.model = None\n",
        "        self.scaler = RobustScaler()\n",
        "        self.threshold = None\n",
        "\n",
        "    def build_model(self):\n",
        "        if not KERAS_AVAILABLE:\n",
        "            raise RuntimeError(\"TensorFlow not available\")\n",
        "\n",
        "        input_layer = layers.Input(shape=(self.seq_len, 1))\n",
        "\n",
        "        # Encoder\n",
        "        x = layers.Conv1D(32, 3, activation='relu', padding='same')(input_layer)\n",
        "        x = layers.MaxPooling1D(2, padding='same')(x)\n",
        "        x = layers.Conv1D(16, 3, activation='relu', padding='same')(x)\n",
        "        encoded = layers.MaxPooling1D(2, padding='same')(x)\n",
        "\n",
        "        # Decoder\n",
        "        x = layers.Conv1D(16, 3, activation='relu', padding='same')(encoded)\n",
        "        x = layers.UpSampling1D(2)(x)\n",
        "        x = layers.Conv1D(32, 3, activation='relu', padding='same')(x)\n",
        "        x = layers.UpSampling1D(2)(x)\n",
        "        decoded = layers.Conv1D(1, 3, activation='linear', padding='same')(x)\n",
        "\n",
        "        self.model = Model(input_layer, decoded)\n",
        "        self.model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    def fit(self, data: np.ndarray, epochs: int = 50):\n",
        "        data_scaled = self.scaler.fit_transform(data.reshape(-1, 1)).flatten()\n",
        "        sequences = create_sequences(data_scaled, self.seq_len)\n",
        "        X = sequences.reshape(-1, self.seq_len, 1)\n",
        "\n",
        "        if self.model is None:\n",
        "            self.build_model()\n",
        "\n",
        "        self.model.fit(X, X, epochs=epochs, batch_size=32, verbose=0)\n",
        "\n",
        "        recon = self.model.predict(X, verbose=0)\n",
        "        errors = np.mean(np.abs(X - recon), axis=(1, 2))\n",
        "        self.threshold = np.percentile(errors, 99.5)\n",
        "\n",
        "    def score(self, data: np.ndarray) -> np.ndarray:\n",
        "        data_scaled = self.scaler.transform(data.reshape(-1, 1)).flatten()\n",
        "        sequences = create_sequences(data_scaled, self.seq_len)\n",
        "        X = sequences.reshape(-1, self.seq_len, 1)\n",
        "\n",
        "        recon = self.model.predict(X, verbose=0)\n",
        "        errors = np.mean(np.abs(X - recon), axis=(1, 2))\n",
        "        return errors\n",
        "\n",
        "class ARIMADetector:\n",
        "    \"\"\"ARIMA-based detector for temperature (slow trends).\"\"\"\n",
        "    def __init__(self, order=(2, 1, 2)):\n",
        "        self.order = order\n",
        "        self.model = None\n",
        "        self.threshold = None\n",
        "\n",
        "    def fit(self, data: np.ndarray):\n",
        "        if not STATSMODELS_AVAILABLE:\n",
        "            print(\"ARIMA unavailable, using simple moving average\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.model = ARIMA(data, order=self.order).fit()\n",
        "            residuals = np.abs(self.model.resid)\n",
        "            self.threshold = np.percentile(residuals, 99.5)\n",
        "        except Exception as e:\n",
        "            print(f\"ARIMA fit failed: {e}, using fallback\")\n",
        "            self.model = None\n",
        "\n",
        "    def score(self, data: np.ndarray) -> np.ndarray:\n",
        "        if self.model is None:\n",
        "            # Fallback: simple moving average residuals\n",
        "            ma = pd.Series(data).rolling(10, min_periods=1).mean().values\n",
        "            return np.abs(data - ma)\n",
        "\n",
        "        try:\n",
        "            forecast = self.model.forecast(steps=len(data))\n",
        "            return np.abs(data - forecast)\n",
        "        except:\n",
        "            ma = pd.Series(data).rolling(10, min_periods=1).mean().values\n",
        "            return np.abs(data - ma)"
      ],
      "metadata": {
        "id": "29w66iW7Urxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# ENSEMBLE SYSTEM\n",
        "# ==============================================================================\n",
        "class EnsembleAnomalyDetector:\n",
        "    \"\"\"Ensemble detector with sensor-type-specific models.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.scalers = {}\n",
        "        self.thresholds = {}\n",
        "\n",
        "    def fit_sensor_type(self, sensor_type: str, data: pd.DataFrame):\n",
        "        \"\"\"Fit appropriate model for sensor type.\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Training model for: {sensor_type}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        values = data['value'].values\n",
        "\n",
        "        if sensor_type == 'strain_gauge':\n",
        "            # Isolation Forest for explainability\n",
        "            feats = make_rolling_features(data, window='30s', step='15s')\n",
        "            feature_cols = [c for c in feats.columns if c.startswith('f_') or c.startswith('ctx_')]\n",
        "\n",
        "            scaler = RobustScaler()\n",
        "            X_scaled = scaler.fit_transform(feats[feature_cols])\n",
        "\n",
        "            model = IsolationForest(n_estimators=300, contamination=0.005, random_state=42, n_jobs=-1)\n",
        "            model.fit(X_scaled)\n",
        "            scores = model.decision_function(X_scaled)\n",
        "            threshold = np.percentile(scores, 0.5)\n",
        "\n",
        "            self.models[sensor_type] = model\n",
        "            self.scalers[sensor_type] = scaler\n",
        "            self.thresholds[sensor_type] = threshold\n",
        "            print(f\"✓ Isolation Forest trained (threshold: {threshold:.4f})\")\n",
        "\n",
        "        elif sensor_type == 'accelerometer_rms':\n",
        "            if KERAS_AVAILABLE:\n",
        "                # 1D CNN for oscillations\n",
        "                model = CNN1D(seq_len=100)\n",
        "                model.fit(values, epochs=30)\n",
        "                self.models[sensor_type] = model\n",
        "                print(f\"✓ 1D CNN trained (threshold: {model.threshold:.6f})\")\n",
        "            else:\n",
        "                # Fallback to Isolation Forest\n",
        "                feats = make_rolling_features(data, window='10s', step='5s')\n",
        "                feature_cols = [c for c in feats.columns if c.startswith('f_')]\n",
        "                scaler = RobustScaler()\n",
        "                X_scaled = scaler.fit_transform(feats[feature_cols])\n",
        "                model = IsolationForest(n_estimators=200, contamination=0.01, random_state=42)\n",
        "                model.fit(X_scaled)\n",
        "                scores = model.decision_function(X_scaled)\n",
        "                threshold = np.percentile(scores, 1)\n",
        "                self.models[sensor_type] = model\n",
        "                self.scalers[sensor_type] = scaler\n",
        "                self.thresholds[sensor_type] = threshold\n",
        "                print(f\"✓ Isolation Forest (fallback) trained\")\n",
        "\n",
        "        elif sensor_type == 'temperature':\n",
        "            # ARIMA for slow trends\n",
        "            model = ARIMADetector(order=(2, 1, 2))\n",
        "            model.fit(values)\n",
        "            self.models[sensor_type] = model\n",
        "            print(f\"✓ ARIMA trained\")\n",
        "\n",
        "    def score_sensor(self, sensor_type: str, data: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Score data for given sensor type.\"\"\"\n",
        "        if sensor_type not in self.models:\n",
        "            print(f\"Warning: No model for {sensor_type}\")\n",
        "            return np.zeros(len(data))\n",
        "\n",
        "        model = self.models[sensor_type]\n",
        "\n",
        "        if sensor_type == 'strain_gauge':\n",
        "            feats = make_rolling_features(data, window='30s', step='15s')\n",
        "            feature_cols = [c for c in feats.columns if c.startswith('f_') or c.startswith('ctx_')]\n",
        "            X_scaled = self.scalers[sensor_type].transform(feats[feature_cols])\n",
        "            scores = model.decision_function(X_scaled)\n",
        "            # Convert to anomaly scores (higher = more anomalous)\n",
        "            return -scores\n",
        "\n",
        "        elif sensor_type == 'accelerometer_rms':\n",
        "            if isinstance(model, CNN1D):\n",
        "                return model.score(data['value'].values)\n",
        "            else:\n",
        "                feats = make_rolling_features(data, window='10s', step='5s')\n",
        "                feature_cols = [c for c in feats.columns if c.startswith('f_')]\n",
        "                X_scaled = self.scalers[sensor_type].transform(feats[feature_cols])\n",
        "                scores = model.decision_function(X_scaled)\n",
        "                return -scores\n",
        "\n",
        "        elif sensor_type == 'temperature':\n",
        "            return model.score(data['value'].values)\n",
        "\n",
        "        return np.zeros(len(data))\n",
        "\n",
        "    def detect_anomalies(self, sensor_type: str, data: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Detect anomalies with sensor-specific threshold.\"\"\"\n",
        "        scores = self.score_sensor(sensor_type, data)\n",
        "\n",
        "        if sensor_type == 'strain_gauge':\n",
        "            threshold = self.thresholds.get(sensor_type, 0)\n",
        "            return (scores > -threshold).astype(int)\n",
        "        elif isinstance(self.models.get(sensor_type), (CNN1D, LSTMAutoencoder)):\n",
        "            threshold = self.models[sensor_type].threshold\n",
        "            return (scores > threshold).astype(int)\n",
        "        elif isinstance(self.models.get(sensor_type), ARIMADetector):\n",
        "            threshold = self.models[sensor_type].threshold or np.percentile(scores, 99.5)\n",
        "            return (scores > threshold).astype(int)\n",
        "        else:\n",
        "            threshold = np.percentile(scores, 99.5)\n",
        "            return (scores > threshold).astype(int)"
      ],
      "metadata": {
        "id": "Swri4tM8Ur16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    df = load_df_with_datetimeindex(os.path.join(DATA_PATH, \"synthetic_bridge_data.csv\"))\n",
        "    print(f\"Loaded {len(df)} rows\")\n",
        "    print(f\"Sensor types: {df['sensor_type'].unique()}\")\n",
        "\n",
        "    # Initialize ensemble\n",
        "    ensemble = EnsembleAnomalyDetector()\n",
        "\n",
        "    # Train models for each sensor type\n",
        "    for sensor_type in df['sensor_type'].unique():\n",
        "        sensor_data = df[df['sensor_type'] == sensor_type].copy()\n",
        "\n",
        "        # Use first sensor as representative\n",
        "        first_sensor = sensor_data['sensor_id'].iloc[0]\n",
        "        train_data = sensor_data[sensor_data['sensor_id'] == first_sensor].iloc[:int(len(sensor_data)*0.7)]\n",
        "\n",
        "        ensemble.fit_sensor_type(sensor_type, train_data)\n",
        "\n",
        "    # Save ensemble\n",
        "    joblib.dump(ensemble, os.path.join(ARTIFACT_DIR, \"ensemble_detector.pkl\"))\n",
        "    print(f\"\\n✓ Ensemble saved to {ARTIFACT_DIR}\")\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EVALUATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    results = {}\n",
        "    for sensor_type in df['sensor_type'].unique():\n",
        "        sensor_data = df[df['sensor_type'] == sensor_type].copy()\n",
        "        first_sensor = sensor_data['sensor_id'].iloc[0]\n",
        "        test_data = sensor_data[sensor_data['sensor_id'] == first_sensor].iloc[int(len(sensor_data)*0.7):]\n",
        "\n",
        "        if len(test_data) < 100:\n",
        "            continue\n",
        "\n",
        "        alerts = ensemble.detect_anomalies(sensor_type, test_data)\n",
        "\n",
        "        if 'anomaly' in test_data.columns:\n",
        "            y_true = test_data['anomaly'].values\n",
        "            if len(alerts) == len(y_true):\n",
        "                p, r, f1, _ = precision_recall_fscore_support(y_true, alerts, average='binary', zero_division=0)\n",
        "                results[sensor_type] = {'precision': p, 'recall': r, 'f1': f1}\n",
        "                print(f\"\\n{sensor_type}:\")\n",
        "                print(f\"  Precision: {p:.3f}  Recall: {r:.3f}  F1: {f1:.3f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Models saved to: {ARTIFACT_DIR}\")\n",
        "    print(\"Artifacts:\")\n",
        "    print(\"  - ensemble_detector.pkl\")\n",
        "    print(\"\\nNext steps:\")\n",
        "    print(\"  1. Deploy ensemble to production pipeline\")\n",
        "    print(\"  2. Monitor model drift with rolling statistics\")\n",
        "    print(\"  3. Implement score fusion for multi-sensor alerts\")\n",
        "    print(\"  4. Set up real-time Streamlit dashboard\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "R3iqlYs9UWdV",
        "outputId": "68c369c8-331b-479b-846a-c9aa926c92d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/ComplexSystemDesign/Data/synthetic_bridge_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3395740572.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_df_with_datetimeindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"synthetic_bridge_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded {len(df)} rows\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Sensor types: {df['sensor_type'].unique()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2917066521.py\u001b[0m in \u001b[0;36mload_df_with_datetimeindex\u001b[0;34m(path, time_col)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatetimeIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ComplexSystemDesign/Data/synthetic_bridge_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Qooz__6Ur23"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}