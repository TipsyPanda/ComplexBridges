{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IsolationForest Validation \u2014 Strain Gauges\n",
        "\n",
        "**Purpose:** Chronologically split strain-gauge features, train an Isolation Forest on earlier data, validate on later data, and save metrics, plots, and predictions per span.\n",
        "\n",
        "**Outputs (per span):**\n",
        "- `isoforest_model.pkl` (trained model)\n",
        "- `threshold.json` (chosen decision threshold and stats)\n",
        "- `validation_plots.png` (distributions, ROC, PR, CM)\n",
        "- `test_predictions.csv` (scores, predicted anomalies, true labels if present)\n",
        "- `validation_results.json` (scalar metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Setup (optional)\n",
        "Uncomment the line below if you're running on Colab or a fresh environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If running on Colab, uncomment:\n",
        "# !pip install -q pandas numpy scikit-learn matplotlib seaborn pyarrow fastparquet joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Imports & Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, joblib, numpy as np, pandas as pd\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_curve, average_precision_score, \n",
        "    precision_recall_fscore_support, roc_auc_score, roc_curve,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "# Detect if running in Google Colab and mount Drive\n",
        "IN_COLAB = False\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    drive = None\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    DATA_PATH = \"/content/drive/MyDrive/ComplexBridge_work/Data/\"\n",
        "    ARTIFACT_DIR = \"/content/drive/MyDrive/ComplexBridge_work/Artifacts/\"\n",
        "else:\n",
        "    # Fallback local paths; adjust if needed\n",
        "    DATA_PATH = \"./Data/\"\n",
        "    ARTIFACT_DIR = \"./Artifacts/\"\n",
        "\n",
        "VALIDATION_DIR = os.path.join(ARTIFACT_DIR, \"validation_results\")\n",
        "os.makedirs(VALIDATION_DIR, exist_ok=True)\n",
        "\n",
        "print(\"DATA_PATH       :\", DATA_PATH)\n",
        "print(\"ARTIFACT_DIR    :\", ARTIFACT_DIR)\n",
        "print(\"VALIDATION_DIR  :\", VALIDATION_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature columns for strain gauge\n",
        "STRAIN_FEATURES = [\"f_mean\",\"f_std\",\"f_rms\",\"f_p2p\",\"f_slope\",\"fft_low\",\"fft_mid\",\"fft_high\"]\n",
        "\n",
        "# Train/test split ratio (chronological split)\n",
        "TRAIN_RATIO = 0.7\n",
        "\n",
        "# Isolation Forest parameters\n",
        "ISO_PARAMS = {\n",
        "    'n_estimators': 300,\n",
        "    'contamination': 'auto',\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_json(path, obj):\n",
        "    \"\"\"Save dictionary to JSON file\"\"\"\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(obj, f, indent=2)\n",
        "\n",
        "def load_json(path):\n",
        "    \"\"\"Load JSON file\"\"\"\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def create_span_dir(span_id: str) -> str:\n",
        "    \"\"\"Create and return validation directory for span\"\"\"\n",
        "    d = os.path.join(VALIDATION_DIR, f\"{span_id}_strain_gauge\")\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "    return d\n",
        "\n",
        "def read_feats(sensor_type: str, use_scaled: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"Read feature files (prefer scaled, prefer parquet)\"\"\"\n",
        "    bases = []\n",
        "    if use_scaled:\n",
        "        bases.append(os.path.join(ARTIFACT_DIR, f\"features/feats_{sensor_type}_scaled\"))\n",
        "    bases.append(os.path.join(ARTIFACT_DIR, f\"features/feats_{sensor_type}\"))\n",
        "    \n",
        "    for base in bases:\n",
        "        for ext in [\".parquet\", \".csv\"]:\n",
        "            path = base + ext\n",
        "            if os.path.exists(path):\n",
        "                if ext == \".parquet\":\n",
        "                    df = pd.read_parquet(path)\n",
        "                else:\n",
        "                    df = pd.read_csv(path)\n",
        "                \n",
        "                # Ensure datetime\n",
        "                if 't_center' in df.columns and not np.issubdtype(df['t_center'].dtype, np.datetime64):\n",
        "                    df['t_center'] = pd.to_datetime(df['t_center'], errors='coerce')\n",
        "                return df\n",
        "    \n",
        "    return pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Load Strain Gauge Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading strain gauge features...\")\n",
        "feats_strain = read_feats(\"strain_gauge\", use_scaled=True)\n",
        "\n",
        "if feats_strain.empty:\n",
        "    print(\"ERROR: No strain gauge features found!\")\n",
        "else:\n",
        "    print(f\"Loaded strain features: shape={feats_strain.shape}\")\n",
        "    print(f\"Columns: {list(feats_strain.columns)}\")\n",
        "    if 't_center' in feats_strain.columns:\n",
        "        print(f\"Date range: {feats_strain['t_center'].min()} to {feats_strain['t_center'].max()}\")\n",
        "    if 'span_id' in feats_strain.columns:\n",
        "        print(f\"Spans: {feats_strain['span_id'].unique().tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Train/Test Split (Chronological)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_train_test_chronological(df: pd.DataFrame, train_ratio: float = 0.7):\n",
        "    \"\"\"\n",
        "    Split dataframe chronologically based on t_center\n",
        "    Returns: train_df, test_df, split_date\n",
        "    \"\"\"\n",
        "    df_sorted = df.sort_values('t_center').reset_index(drop=True)\n",
        "    split_idx = int(len(df_sorted) * train_ratio)\n",
        "    \n",
        "    train_df = df_sorted.iloc[:split_idx].copy()\n",
        "    test_df = df_sorted.iloc[split_idx:].copy()\n",
        "    split_date = train_df['t_center'].max()\n",
        "    \n",
        "    return train_df, test_df, split_date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Visualization Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_validation_results(span_id, save_dir, train_scores, test_scores, y_test, \n",
        "                            threshold, train_df, test_df):\n",
        "    \"\"\"Create comprehensive validation plots\"\"\"\n",
        "    \n",
        "    fig = plt.figure(figsize=(16, 12))\n",
        "    \n",
        "    # 1. Score distributions\n",
        "    ax1 = plt.subplot(3, 2, 1)\n",
        "    plt.hist(train_scores, bins=50, alpha=0.6, label='Train', density=True)\n",
        "    plt.hist(test_scores, bins=50, alpha=0.6, label='Test', density=True)\n",
        "    plt.axvline(threshold, color='r', linestyle='--', label=f'Threshold={threshold:.4f}')\n",
        "    plt.xlabel('Anomaly Score')\n",
        "    plt.ylabel('Density')\n",
        "    plt.title('Score Distribution (Train vs Test)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Score distribution by true label\n",
        "    ax2 = plt.subplot(3, 2, 2)\n",
        "    normal_scores = test_scores[y_test == 0]\n",
        "    anomaly_scores = test_scores[y_test == 1]\n",
        "    plt.hist(normal_scores, bins=30, alpha=0.6, label=f'Normal (n={len(normal_scores)})', density=True)\n",
        "    plt.hist(anomaly_scores, bins=30, alpha=0.6, label=f'Anomaly (n={len(anomaly_scores)})', density=True)\n",
        "    plt.axvline(threshold, color='r', linestyle='--', label='Threshold')\n",
        "    plt.xlabel('Anomaly Score')\n",
        "    plt.ylabel('Density')\n",
        "    plt.title('Test Set: Scores by True Label')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Time series of scores\n",
        "    ax3 = plt.subplot(3, 2, 3)\n",
        "    plt.scatter(test_df['t_center'], test_scores, c=y_test, cmap='coolwarm', \n",
        "               alpha=0.6, s=10, label='Test samples')\n",
        "    plt.axhline(threshold, color='r', linestyle='--', label='Threshold')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Anomaly Score')\n",
        "    plt.title('Anomaly Scores Over Time (Test Set)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    # 4. ROC Curve\n",
        "    ax4 = plt.subplot(3, 2, 4)\n",
        "    try:\n",
        "        fpr, tpr, _ = roc_curve(y_test, -test_scores)\n",
        "        roc_auc = roc_auc_score(y_test, -test_scores)\n",
        "        plt.plot(fpr, tpr, label=f'ROC (AUC={roc_auc:.3f})')\n",
        "        plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('ROC Curve')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "    except Exception as e:\n",
        "        plt.text(0.5, 0.5, f'ROC plot failed:\\n{str(e)}', ha='center', va='center')\n",
        "    \n",
        "    # 5. Precision-Recall Curve\n",
        "    ax5 = plt.subplot(3, 2, 5)\n",
        "    try:\n",
        "        precision, recall, _ = precision_recall_curve(y_test, -test_scores)\n",
        "        avg_precision = average_precision_score(y_test, -test_scores)\n",
        "        plt.plot(recall, precision, label=f'PR (AP={avg_precision:.3f})')\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.title('Precision-Recall Curve')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "    except Exception as e:\n",
        "        plt.text(0.5, 0.5, f'PR plot failed:\\n{str(e)}', ha='center', va='center')\n",
        "    \n",
        "    # 6. Confusion Matrix\n",
        "    ax6 = plt.subplot(3, 2, 6)\n",
        "    test_pred = (test_scores < threshold).astype(int)\n",
        "    cm = confusion_matrix(y_test, test_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=['Normal', 'Anomaly'],\n",
        "                yticklabels=['Normal', 'Anomaly'])\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.title('Confusion Matrix (Test Set)')\n",
        "    \n",
        "    plt.suptitle(f'Validation Results: {span_id}', fontsize=16, y=0.995)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    plot_path = os.path.join(save_dir, 'validation_plots.png')\n",
        "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "    print(f\"Plots saved to: {plot_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Model Training & Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_validate_isoforest(span_id: str, df_span: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Train Isolation Forest on training data and validate on test data\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing Span: {span_id}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    val_dir = create_span_dir(span_id)\n",
        "    \n",
        "    # Check if we have all required features\n",
        "    missing_feats = [f for f in STRAIN_FEATURES if f not in df_span.columns]\n",
        "    if missing_feats:\n",
        "        print(f\"ERROR: Missing features: {missing_feats}\")\n",
        "        return None\n",
        "    \n",
        "    # Split train/test chronologically\n",
        "    train_df, test_df, split_date = split_train_test_chronological(df_span, TRAIN_RATIO)\n",
        "    \n",
        "    print(f\"\\nData Split:\")\n",
        "    print(f\"  Total samples: {len(df_span)}\")\n",
        "    print(f\"  Train samples: {len(train_df)} ({len(train_df)/len(df_span)*100:.1f}%)\")\n",
        "    print(f\"  Test samples:  {len(test_df)} ({len(test_df)/len(df_span)*100:.1f}%)\")\n",
        "    print(f\"  Split date:    {split_date}\")\n",
        "    \n",
        "    # Prepare feature matrices\n",
        "    X_train = train_df[STRAIN_FEATURES].to_numpy()\n",
        "    X_test = test_df[STRAIN_FEATURES].to_numpy()\n",
        "    \n",
        "    # Check for labels if available\n",
        "    has_labels = 'label_rule' in df_span.columns\n",
        "    if has_labels:\n",
        "        y_train = train_df['label_rule'].to_numpy()\n",
        "        y_test = test_df['label_rule'].to_numpy()\n",
        "        print(f\"\\nLabels found:\")\n",
        "        print(f\"  Train anomalies: {y_train.sum()} ({y_train.sum()/len(y_train)*100:.2f}%)\")\n",
        "        print(f\"  Test anomalies:  {y_test.sum()} ({y_test.sum()/len(y_test)*100:.2f}%)\")\n",
        "    \n",
        "    # Train Isolation Forest\n",
        "    print(f\"\\nTraining Isolation Forest...\")\n",
        "    iso_model = IsolationForest(**ISO_PARAMS)\n",
        "    iso_model.fit(X_train)\n",
        "    \n",
        "    # Get anomaly scores (higher = more normal; lower = more anomalous)\n",
        "    train_scores = iso_model.decision_function(X_train)\n",
        "    test_scores = iso_model.decision_function(X_test)\n",
        "    \n",
        "    print(f\"  Train score range: [{train_scores.min():.4f}, {train_scores.max():.4f}]\")\n",
        "    print(f\"  Test score range:  [{test_scores.min():.4f}, {test_scores.max():.4f}]\")\n",
        "    \n",
        "    # Determine threshold (0.5 percentile from training data)\n",
        "    threshold = float(np.percentile(train_scores, 0.5))\n",
        "    print(f\"  Threshold (0.5 percentile): {threshold:.6f}\")\n",
        "    \n",
        "    # Predict anomalies\n",
        "    train_pred = (train_scores < threshold).astype(int)\n",
        "    test_pred = (test_scores < threshold).astype(int)\n",
        "    \n",
        "    print(f\"\\nPredicted anomalies:\")\n",
        "    print(f\"  Train: {train_pred.sum()} ({train_pred.sum()/len(train_pred)*100:.2f}%)\")\n",
        "    print(f\"  Test:  {test_pred.sum()} ({test_pred.sum()/len(test_pred)*100:.2f}%)\")\n",
        "    \n",
        "    # Save model and threshold\n",
        "    model_path = os.path.join(val_dir, \"isoforest_model.pkl\")\n",
        "    joblib.dump(iso_model, model_path)\n",
        "    \n",
        "    threshold_path = os.path.join(val_dir, \"threshold.json\")\n",
        "    save_json(threshold_path, {\n",
        "        \"score_threshold\": threshold,\n",
        "        \"train_score_mean\": float(train_scores.mean()),\n",
        "        \"train_score_std\": float(train_scores.std()),\n",
        "        \"split_date\": str(split_date),\n",
        "        \"note\": \"score < threshold => anomaly\"\n",
        "    })\n",
        "    \n",
        "    # Evaluation metrics\n",
        "    results = {\n",
        "        \"span_id\": span_id,\n",
        "        \"split_date\": str(split_date),\n",
        "        \"train_samples\": int(len(train_df)),\n",
        "        \"test_samples\": int(len(test_df)),\n",
        "        \"threshold\": float(threshold),\n",
        "        \"train_anomaly_rate\": float(train_pred.sum() / len(train_pred)),\n",
        "        \"test_anomaly_rate\": float(test_pred.sum() / len(test_pred)),\n",
        "    }\n",
        "    \n",
        "    # If we have labels, compute classification metrics\n",
        "    if has_labels:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"CLASSIFICATION METRICS (Test Set)\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_test, test_pred)\n",
        "        print(f\"\\nConfusion Matrix:\")\n",
        "        print(f\"                Predicted Normal  Predicted Anomaly\")\n",
        "        print(f\"Actual Normal   {cm[0,0]:15d}  {cm[0,1]:17d}\")\n",
        "        print(f\"Actual Anomaly  {cm[1,0]:15d}  {cm[1,1]:17d}\")\n",
        "        \n",
        "        # Metrics\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, test_pred, average='binary')\n",
        "        \n",
        "        print(f\"\\nPerformance Metrics:\")\n",
        "        print(f\"  Precision: {precision:.4f}\")\n",
        "        print(f\"  Recall:    {recall:.4f}\")\n",
        "        print(f\"  F1-Score:  {f1:.4f}\")\n",
        "        \n",
        "        # ROC-AUC (using negative scores since lower = more anomalous)\n",
        "        try:\n",
        "            roc_auc = roc_auc_score(y_test, -test_scores)\n",
        "            print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
        "        except:\n",
        "            roc_auc = None\n",
        "        \n",
        "        # Average Precision\n",
        "        try:\n",
        "            avg_precision = average_precision_score(y_test, -test_scores)\n",
        "            print(f\"  Avg Precision: {avg_precision:.4f}\")\n",
        "        except:\n",
        "            avg_precision = None\n",
        "        \n",
        "        # Add to results\n",
        "        results.update({\n",
        "            \"test_precision\": float(precision),\n",
        "            \"test_recall\": float(recall),\n",
        "            \"test_f1\": float(f1),\n",
        "            \"test_roc_auc\": float(roc_auc) if roc_auc else None,\n",
        "            \"test_avg_precision\": float(avg_precision) if avg_precision else None,\n",
        "            \"confusion_matrix\": cm.tolist(),\n",
        "            \"true_anomalies\": int(y_test.sum()),\n",
        "            \"detected_anomalies\": int(test_pred.sum()),\n",
        "            \"true_positives\": int(cm[1,1]),\n",
        "            \"false_positives\": int(cm[0,1]),\n",
        "            \"true_negatives\": int(cm[0,0]),\n",
        "            \"false_negatives\": int(cm[1,0])\n",
        "        })\n",
        "        \n",
        "        # Plot results\n",
        "        plot_validation_results(\n",
        "            span_id, val_dir, \n",
        "            train_scores, test_scores, y_test, \n",
        "            threshold, train_df, test_df\n",
        "        )\n",
        "    \n",
        "    # Save results\n",
        "    results_path = os.path.join(val_dir, \"validation_results.json\")\n",
        "    save_json(results_path, results)\n",
        "    \n",
        "    # Save predictions\n",
        "    test_results_df = test_df.copy()\n",
        "    test_results_df['anomaly_score'] = test_scores\n",
        "    test_results_df['predicted_anomaly'] = test_pred\n",
        "    if has_labels:\n",
        "        test_results_df['true_label'] = y_test\n",
        "    \n",
        "    pred_path = os.path.join(val_dir, \"test_predictions.csv\")\n",
        "    test_results_df.to_csv(pred_path, index=False)\n",
        "    \n",
        "    print(f\"\\nResults saved to: {val_dir}\")\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Run Validation for All Spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not feats_strain.empty and 'span_id' in feats_strain.columns:\n",
        "    spans = feats_strain['span_id'].dropna().unique().tolist()\n",
        "    print(f\"\\nFound {len(spans)} span(s): {spans}\")\n",
        "    \n",
        "    all_results = []\n",
        "    \n",
        "    for span_id in spans:\n",
        "        try:\n",
        "            # Filter data for this span\n",
        "            df_span = feats_strain[\n",
        "                (feats_strain['span_id'] == span_id) & \n",
        "                (feats_strain['sensor_type'] == 'strain_gauge')\n",
        "            ].copy()\n",
        "            \n",
        "            if df_span.empty:\n",
        "                print(f\"\\n[{span_id}] No strain gauge data found - skipping\")\n",
        "                continue\n",
        "            \n",
        "            if len(df_span) < 100:\n",
        "                print(f\"\\n[{span_id}] Insufficient data ({len(df_span)} samples) - skipping\")\n",
        "                continue\n",
        "            \n",
        "            # Train and validate\n",
        "            result = train_and_validate_isoforest(span_id, df_span)\n",
        "            \n",
        "            if result:\n",
        "                all_results.append(result)\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"\\n[{span_id}] ERROR: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    \n",
        "    # Save summary\n",
        "    if all_results:\n",
        "        summary_path = os.path.join(VALIDATION_DIR, \"validation_summary.json\")\n",
        "        save_json(summary_path, {\n",
        "            \"validation_date\": datetime.now().isoformat(),\n",
        "            \"n_spans\": len(all_results),\n",
        "            \"results\": all_results\n",
        "        })\n",
        "        \n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"VALIDATION SUMMARY\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Successfully validated {len(all_results)} span(s)\")\n",
        "        print(f\"Summary saved to: {summary_path}\")\n",
        "        \n",
        "        # Print summary table (if labels were available)\n",
        "        if all_results and 'test_precision' in all_results[0]:\n",
        "            print(f\"\\n{'Span ID':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'ROC-AUC':<12}\")\n",
        "            print(\"-\" * 63)\n",
        "            for r in all_results:\n",
        "                print(f\"{r['span_id']:<15} {r['test_precision']:<12.4f} {r['test_recall']:<12.4f} \"\n",
        "                      f\"{r['test_f1']:<12.4f} {r.get('test_roc_auc', 'N/A'):<12}\")\n",
        "else:\n",
        "    print(\"ERROR: No span data available for validation\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Validation Complete!\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "name": "IsolationForest_Validation.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
