{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TipsyPanda/ComplexBridges/blob/main/IPMB_Anomaly_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b81a8818",
      "metadata": {
        "id": "b81a8818"
      },
      "source": [
        "# IPMB â€“ ML Anomaly Detection Starter Notebook\n",
        "*Generated on 2025-10-13 13:12:05*\n",
        "\n",
        "This notebook covers:\n",
        "1. Data loading & basic EDA\n",
        "2. Rolling-window feature engineering\n",
        "3. Unsupervised anomaly detection (Isolation Forest)\n",
        "4. Threshold calibration & evaluation (time-aware)\n",
        "5. Minimal, stream-friendly scoring loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79317bfc",
      "metadata": {
        "id": "79317bfc"
      },
      "source": [
        "## 1) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "36f4f5b2",
      "metadata": {
        "id": "36f4f5b2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install (if running elsewhere) and imports\n",
        "# %pip install pandas numpy scikit-learn matplotlib scipy\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import precision_recall_fscore_support, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Matplotlib defaults (no explicit colors)\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
        "\n",
        "# Paths\n",
        "DATA_PATH = \"/mnt/data/ipmb_5sensors_30min_1_to_10hz.csv\"  # change if needed\n",
        "ARTIFACT_DIR = \"/mnt/data/artifacts\"\n",
        "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80c97f52",
      "metadata": {
        "id": "80c97f52"
      },
      "source": [
        "## 2) Load & sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b7edef08",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "b7edef08",
        "outputId": "de5de880-08d5-475a-8308-90093181d2ba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/mnt/data/ipmb_5sensors_30min_1_to_10hz.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3307762539.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Basic parsing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/ipmb_5sensors_30min_1_to_10hz.csv'"
          ]
        }
      ],
      "source": [
        "\n",
        "# Read\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Basic parsing\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "# Expected columns (from project guidelines)\n",
        "expected = [\n",
        "    'timestamp','bridge_id','span_id','sensor_id','sensor_type',\n",
        "    'value','unit','traffic_load_proxy','rule_threshold','anomaly','anomaly_type'\n",
        "]\n",
        "missing = [c for c in expected if c not in df.columns]\n",
        "print(\"Missing columns:\", missing)\n",
        "\n",
        "# Drop rows without timestamp\n",
        "df = df[~df['timestamp'].isna()].copy()\n",
        "\n",
        "# Quick look\n",
        "print(df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "664926ec",
      "metadata": {
        "id": "664926ec"
      },
      "source": [
        "## 3) Basic EDA (per sensor_type / sensor_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "589ed4ab",
      "metadata": {
        "id": "589ed4ab"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Choose a single stream to develop on (best: per sensor_id Ã— span_id)\n",
        "# You can change these two lines to target a different stream.\n",
        "example_type = df['sensor_type'].dropna().unique()[0]\n",
        "example_sensor = df.loc[df['sensor_type']==example_type, 'sensor_id'].dropna().unique()[0]\n",
        "\n",
        "sub = df[(df['sensor_type']==example_type) & (df['sensor_id']==example_sensor)].copy()\n",
        "sub = sub.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "print(\"Working stream:\", example_type, example_sensor, \"rows:\", len(sub))\n",
        "\n",
        "# Plot raw value + rule flags for a quick feel\n",
        "fig = plt.figure()\n",
        "plt.plot(sub['timestamp'], sub['value'], label='value')\n",
        "plt.title(f\"Raw values â€“ {example_sensor} ({example_type})\")\n",
        "plt.xlabel(\"time\"); plt.ylabel(\"value\")\n",
        "plt.show()\n",
        "\n",
        "# Overlay anomaly flags (rule-based)\n",
        "if 'anomaly' in sub.columns:\n",
        "    fig = plt.figure()\n",
        "    plt.plot(sub['timestamp'], sub['value'])\n",
        "    # annotate anomalies by vertical lines\n",
        "    t_anom = sub.loc[sub['anomaly']==1, 'timestamp']\n",
        "    for t in t_anom:\n",
        "        plt.axvline(t, linestyle='--', linewidth=0.8)\n",
        "    plt.title(f\"Rule anomalies overlay â€“ {example_sensor}\")\n",
        "    plt.xlabel(\"time\"); plt.ylabel(\"value\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e384f2c5",
      "metadata": {
        "id": "e384f2c5"
      },
      "source": [
        "## 4) Rolling-window feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd325899",
      "metadata": {
        "id": "fd325899"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Helper: generate rolling-window features for a single stream\n",
        "def make_rolling_features(ts: pd.DataFrame, window='30s', step='15s'):\n",
        "    # Ensure fixed frequency via resampling (forward-fill short gaps)\n",
        "    ts = ts.set_index('timestamp').sort_index()\n",
        "    # Infer a sensible base frequency\n",
        "    base_freq = pd.infer_freq(ts.index)\n",
        "    if base_freq is None:\n",
        "        # fallback: use median delta\n",
        "        deltas = ts.index.to_series().diff().dropna().dt.total_seconds()\n",
        "        nominal = f\"{int(np.median(deltas))}S\"\n",
        "        base_freq = nominal\n",
        "    ts = ts.resample(base_freq).ffill()\n",
        "\n",
        "    # Create rolling windows: we emulate a sliding approach by stepping\n",
        "    start = ts.index.min()\n",
        "    end = ts.index.max()\n",
        "    out_rows = []\n",
        "    cur = start\n",
        "    while cur + pd.to_timedelta(window) <= end:\n",
        "        win = ts.loc[cur:cur+pd.to_timedelta(window)]\n",
        "        if len(win) < 3:\n",
        "            cur += pd.to_timedelta(step)\n",
        "            continue\n",
        "        v = win['value'].values\n",
        "\n",
        "        # time-domain stats\n",
        "        mean = np.mean(v)\n",
        "        std = np.std(v, ddof=1) if len(v) > 1 else 0.0\n",
        "        rms = math.sqrt(np.mean(v**2))\n",
        "        p2p = np.max(v) - np.min(v)\n",
        "        skew = pd.Series(v).skew()\n",
        "        kurt = pd.Series(v).kurtosis()\n",
        "\n",
        "        # trend (slope) via simple linear regression on index steps\n",
        "        x = np.arange(len(v))\n",
        "        if len(v) > 1:\n",
        "            slope = np.polyfit(x, v, 1)[0]\n",
        "        else:\n",
        "            slope = 0.0\n",
        "\n",
        "        # diffs\n",
        "        dv = np.diff(v)\n",
        "        adiff_mean = np.mean(np.abs(dv)) if len(dv) else 0.0\n",
        "\n",
        "        # context features (if present)\n",
        "        ctx = {}\n",
        "        if 'traffic_load_proxy' in win.columns:\n",
        "            tl = win['traffic_load_proxy'].values\n",
        "            ctx['ctx_tl_mean'] = float(np.mean(tl))\n",
        "            ctx['ctx_tl_std'] = float(np.std(tl, ddof=1)) if len(tl)>1 else 0.0\n",
        "        if 'rule_threshold' in win.columns:\n",
        "            ctx['ctx_rule_thr'] = float(np.median(win['rule_threshold']))\n",
        "\n",
        "        # label (weak): any rule anomaly in the window -> 1\n",
        "        label = None\n",
        "        if 'anomaly' in win.columns:\n",
        "            label = int((win['anomaly'] == 1).any())\n",
        "        # window center time\n",
        "        t_center = cur + (pd.to_timedelta(window)/2)\n",
        "\n",
        "        row = {\n",
        "            't_center': t_center,\n",
        "            'f_mean': float(mean),\n",
        "            'f_std': float(std),\n",
        "            'f_rms': float(rms),\n",
        "            'f_p2p': float(p2p),\n",
        "            'f_skew': float(skew) if not np.isnan(skew) else 0.0,\n",
        "            'f_kurt': float(kurt) if not np.isnan(kurt) else 0.0,\n",
        "            'f_slope': float(slope),\n",
        "            'f_adiff_mean': float(adiff_mean),\n",
        "            **ctx\n",
        "        }\n",
        "        if label is not None:\n",
        "            row['label_rule'] = label\n",
        "\n",
        "        out_rows.append(row)\n",
        "        cur += pd.to_timedelta(step)\n",
        "\n",
        "    feats = pd.DataFrame(out_rows).sort_values('t_center').reset_index(drop=True)\n",
        "    return feats\n",
        "\n",
        "feats = make_rolling_features(sub[['timestamp','value','traffic_load_proxy','rule_threshold','anomaly']].copy(),\n",
        "                              window='30s', step='15s')\n",
        "print(feats.shape)\n",
        "feats.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80c20362",
      "metadata": {
        "id": "80c20362"
      },
      "source": [
        "## 5) Temporal split and scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "746cc5ff",
      "metadata": {
        "id": "746cc5ff"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Temporal split (70/30)\n",
        "cut = int(len(feats)*0.7)\n",
        "train = feats.iloc[:cut].copy()\n",
        "test  = feats.iloc[cut:].copy()\n",
        "\n",
        "feature_cols = [c for c in feats.columns if c.startswith('f_') or c.startswith('ctx_')]\n",
        "label_col = 'label_rule' if 'label_rule' in feats.columns else None\n",
        "\n",
        "scaler = RobustScaler()\n",
        "Xs_tr = scaler.fit_transform(train[feature_cols])\n",
        "Xs_te = scaler.transform(test[feature_cols])\n",
        "\n",
        "# Save scaler\n",
        "import joblib\n",
        "joblib.dump(scaler, os.path.join(ARTIFACT_DIR, \"scaler.joblib\"))\n",
        "print(\"Saved:\", os.path.join(ARTIFACT_DIR, \"scaler.joblib\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20ae5376",
      "metadata": {
        "id": "20ae5376"
      },
      "source": [
        "## 6) Unsupervised model: Isolation Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ee5d1d",
      "metadata": {
        "id": "d0ee5d1d"
      },
      "outputs": [],
      "source": [
        "\n",
        "iso = IsolationForest(\n",
        "    n_estimators=300,\n",
        "    contamination='auto',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "iso.fit(Xs_tr)\n",
        "\n",
        "# Scores: higher -> more normal in sklearn's decision_function\n",
        "score_tr = iso.decision_function(Xs_tr)\n",
        "score_te = iso.decision_function(Xs_te)\n",
        "\n",
        "# Save model\n",
        "import joblib, json\n",
        "joblib.dump(iso, os.path.join(ARTIFACT_DIR, \"isoforest.pkl\"))\n",
        "print(\"Saved:\", os.path.join(ARTIFACT_DIR, \"isoforest.pkl\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a501520",
      "metadata": {
        "id": "3a501520"
      },
      "source": [
        "## 7) Threshold calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0bcd547",
      "metadata": {
        "id": "a0bcd547"
      },
      "outputs": [],
      "source": [
        "\n",
        "def pick_threshold(scores, quantile=0.005):\n",
        "    # Convert to alerts for the lowest quantile of normality score\n",
        "    thr = np.quantile(scores, quantile)\n",
        "    return float(thr)\n",
        "\n",
        "thr = pick_threshold(score_tr, quantile=0.005)  # ~0.5% most abnormal on train become alerts\n",
        "print(\"Chosen threshold:\", thr)\n",
        "\n",
        "with open(os.path.join(ARTIFACT_DIR, \"threshold.json\"), \"w\") as f:\n",
        "    json.dump({\"score_threshold\": thr}, f)\n",
        "print(\"Saved:\", os.path.join(ARTIFACT_DIR, \"threshold.json\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f72454a",
      "metadata": {
        "id": "1f72454a"
      },
      "source": [
        "## 8) Evaluation (if weak labels exist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb8e3f64",
      "metadata": {
        "id": "bb8e3f64"
      },
      "outputs": [],
      "source": [
        "\n",
        "alerts_te = (score_te < thr).astype(int)\n",
        "\n",
        "if label_col is not None and label_col in test.columns:\n",
        "    y_true = test[label_col].values\n",
        "    # PR metrics (preferred for rare events)\n",
        "    # For AP, we need a continuous score where higher means more likely positive.\n",
        "    # We invert the normality score to an anomaly score:\n",
        "    anomaly_score_te = -score_te\n",
        "    ap = average_precision_score(y_true, anomaly_score_te)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_true, alerts_te, average='binary', zero_division=0)\n",
        "    print(f\"AP: {ap:.3f}  Precision: {p:.3f}  Recall: {r:.3f}  F1: {f1:.3f}\")\n",
        "else:\n",
        "    print(\"No labels available; showing score distribution only.\")\n",
        "\n",
        "# Plot score timeline + threshold\n",
        "fig = plt.figure()\n",
        "plt.plot(test['t_center'], score_te, label='score')\n",
        "plt.axhline(thr, linestyle='--')\n",
        "plt.title(\"IsolationForest score (higher=more normal)\")\n",
        "plt.xlabel(\"time\"); plt.ylabel(\"score\")\n",
        "plt.show()\n",
        "\n",
        "# Alert markers\n",
        "fig = plt.figure()\n",
        "plt.plot(test['t_center'], alerts_te, drawstyle='steps-post')\n",
        "plt.title(\"Alerts (test)\")\n",
        "plt.xlabel(\"time\"); plt.ylabel(\"alert\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "565fa692",
      "metadata": {
        "id": "565fa692"
      },
      "source": [
        "## 9) Mini scoring loop (stream-friendly illustration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0303d87",
      "metadata": {
        "id": "f0303d87"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Pseudostream: iterate windows over the test set and emit events\n",
        "events = []\n",
        "for t, s, a in zip(test['t_center'], score_te, (score_te < thr).astype(int)):\n",
        "    if a == 1:\n",
        "        events.append({\"t\": str(t), \"event\": \"ALERT\", \"score\": float(s)})\n",
        "len(events), events[:5]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54f1c422",
      "metadata": {
        "id": "54f1c422"
      },
      "source": [
        "## 10) Next steps\n",
        "\n",
        "- Expand features (FFT band powers for accel/strain).\n",
        "- Per-sensor models vs. global per-type models.\n",
        "- Cost-aware thresholding (target â‰¤ 1 false alarm/day).\n",
        "- Add drift monitoring (score median + MAD over time).\n",
        "- Integrate with Streamlit dashboard (plot raw signals + score + alerts).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}