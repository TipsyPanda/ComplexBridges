{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TipsyPanda/ComplexBridges/blob/main/IPMB_Anomaly_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b81a8818",
      "metadata": {
        "id": "b81a8818"
      },
      "source": [
        "# IPMB – ML Anomaly Detection Starter Notebook\n",
        "*Generated on 2025-10-13 13:12:05*\n",
        "\n",
        "This notebook covers:\n",
        "1. Data loading & basic EDA\n",
        "2. Rolling-window feature engineering\n",
        "3. Unsupervised anomaly detection (Isolation Forest)\n",
        "4. Threshold calibration & evaluation (time-aware)\n",
        "5. Minimal, stream-friendly scoring loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79317bfc",
      "metadata": {
        "id": "79317bfc"
      },
      "source": [
        "## 1) Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "36f4f5b2",
      "metadata": {
        "id": "36f4f5b2",
        "outputId": "07c9fa5a-7663-409c-cd00-e9ed8161a5a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install (if running elsewhere) and imports\n",
        "# %pip install pandas numpy scikit-learn matplotlib scipy\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import precision_recall_fscore_support, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Matplotlib defaults (no explicit colors)\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "# Paths\n",
        "DATA_PATH = \"/content/drive/MyDrive/ComplexSystemDesign/Data/\"  # change if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80c97f52",
      "metadata": {
        "id": "80c97f52"
      },
      "source": [
        "## 2) Load & sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b7edef08",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7edef08",
        "outputId": "c651c8fe-08a1-47d1-cca3-fa3ecb49e4b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90000, 10)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 90000 entries, 2025-10-10 12:00:00 to 2025-10-10 12:29:59.900000\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   bridge_id           90000 non-null  object \n",
            " 1   span_id             90000 non-null  object \n",
            " 2   sensor_id           90000 non-null  object \n",
            " 3   sensor_type         90000 non-null  object \n",
            " 4   value               90000 non-null  float64\n",
            " 5   unit                90000 non-null  object \n",
            " 6   traffic_load_proxy  90000 non-null  float64\n",
            " 7   rule_threshold      90000 non-null  float64\n",
            " 8   anomaly             90000 non-null  int64  \n",
            " 9   anomaly_type        3852 non-null   object \n",
            "dtypes: float64(3), int64(1), object(6)\n",
            "memory usage: 7.6+ MB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Read\n",
        "df = pd.read_pickle(DATA_PATH+\"DF_BridgeData_cleaned.pkl\")\n",
        "print(df.shape)\n",
        "df.head()\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "# Drop rows without timestamp\n",
        "df = df[~df['timestamp'].isna()].copy()\n",
        "# Quick look"
      ],
      "metadata": {
        "id": "qMAU1NQuD9Za"
      },
      "id": "qMAU1NQuD9Za",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "664926ec",
      "metadata": {
        "id": "664926ec"
      },
      "source": [
        "## 3) Basic EDA (per sensor_type / sensor_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "589ed4ab",
      "metadata": {
        "id": "589ed4ab"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Choose a single stream to develop on (best: per sensor_id × span_id)\n",
        "# You can change these two lines to target a different stream.\n",
        "example_type = df['sensor_type'].dropna().unique()[0]\n",
        "example_sensor = df.loc[df['sensor_type']==example_type, 'sensor_id'].dropna().unique()[0]\n",
        "\n",
        "sub = df[(df['sensor_type']==example_type) & (df['sensor_id']==example_sensor)].copy()\n",
        "sub = sub.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "print(\"Working stream:\", example_type, example_sensor, \"rows:\", len(sub))\n",
        "\n",
        "# Plot raw value + rule flags for a quick feel\n",
        "fig = plt.figure()\n",
        "plt.plot(sub['timestamp'], sub['value'], label='value')\n",
        "plt.title(f\"Raw values – {example_sensor} ({example_type})\")\n",
        "plt.xlabel(\"time\"); plt.ylabel(\"value\")\n",
        "plt.show()\n",
        "\n",
        "# Overlay anomaly flags (rule-based)\n",
        "if 'anomaly' in sub.columns:\n",
        "    fig = plt.figure()\n",
        "    plt.plot(sub['timestamp'], sub['value'])\n",
        "    # annotate anomalies by vertical lines\n",
        "    t_anom = sub.loc[sub['anomaly']==1, 'timestamp']\n",
        "    for t in t_anom:\n",
        "        plt.axvline(t, linestyle='--', linewidth=0.8)\n",
        "    plt.title(f\"Rule anomalies overlay – {example_sensor}\")\n",
        "    plt.xlabel(\"time\"); plt.ylabel(\"value\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e384f2c5",
      "metadata": {
        "id": "e384f2c5"
      },
      "source": [
        "## 4) Rolling-window feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd325899",
      "metadata": {
        "id": "fd325899"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Helper: generate rolling-window features for a single stream\n",
        "def make_rolling_features(ts: pd.DataFrame, window='30s', step='15s'):\n",
        "    # Ensure fixed frequency via resampling (forward-fill short gaps)\n",
        "    ts = ts.set_index('timestamp').sort_index()\n",
        "    # Infer a sensible base frequency\n",
        "    base_freq = pd.infer_freq(ts.index)\n",
        "    if base_freq is None:\n",
        "        # fallback: use median delta\n",
        "        deltas = ts.index.to_series().diff().dropna().dt.total_seconds()\n",
        "        nominal = f\"{int(np.median(deltas))}S\"\n",
        "        base_freq = nominal\n",
        "    ts = ts.resample(base_freq).ffill()\n",
        "\n",
        "    # Create rolling windows: we emulate a sliding approach by stepping\n",
        "    start = ts.index.min()\n",
        "    end = ts.index.max()\n",
        "    out_rows = []\n",
        "    cur = start\n",
        "    while cur + pd.to_timedelta(window) <= end:\n",
        "        win = ts.loc[cur:cur+pd.to_timedelta(window)]\n",
        "        if len(win) < 3:\n",
        "            cur += pd.to_timedelta(step)\n",
        "            continue\n",
        "        v = win['value'].values\n",
        "\n",
        "        # time-domain stats\n",
        "        mean = np.mean(v)\n",
        "        std = np.std(v, ddof=1) if len(v) > 1 else 0.0\n",
        "        rms = math.sqrt(np.mean(v**2))\n",
        "        p2p = np.max(v) - np.min(v)\n",
        "        skew = pd.Series(v).skew()\n",
        "        kurt = pd.Series(v).kurtosis()\n",
        "\n",
        "        # trend (slope) via simple linear regression on index steps\n",
        "        x = np.arange(len(v))\n",
        "        if len(v) > 1:\n",
        "            slope = np.polyfit(x, v, 1)[0]\n",
        "        else:\n",
        "            slope = 0.0\n",
        "\n",
        "        # diffs\n",
        "        dv = np.diff(v)\n",
        "        adiff_mean = np.mean(np.abs(dv)) if len(dv) else 0.0\n",
        "\n",
        "        # context features (if present)\n",
        "        ctx = {}\n",
        "        if 'traffic_load_proxy' in win.columns:\n",
        "            tl = win['traffic_load_proxy'].values\n",
        "            ctx['ctx_tl_mean'] = float(np.mean(tl))\n",
        "            ctx['ctx_tl_std'] = float(np.std(tl, ddof=1)) if len(tl)>1 else 0.0\n",
        "        if 'rule_threshold' in win.columns:\n",
        "            ctx['ctx_rule_thr'] = float(np.median(win['rule_threshold']))\n",
        "\n",
        "        # label (weak): any rule anomaly in the window -> 1\n",
        "        label = None\n",
        "        if 'anomaly' in win.columns:\n",
        "            label = int((win['anomaly'] == 1).any())\n",
        "        # window center time\n",
        "        t_center = cur + (pd.to_timedelta(window)/2)\n",
        "\n",
        "        row = {\n",
        "            't_center': t_center,\n",
        "            'f_mean': float(mean),\n",
        "            'f_std': float(std),\n",
        "            'f_rms': float(rms),\n",
        "            'f_p2p': float(p2p),\n",
        "            'f_skew': float(skew) if not np.isnan(skew) else 0.0,\n",
        "            'f_kurt': float(kurt) if not np.isnan(kurt) else 0.0,\n",
        "            'f_slope': float(slope),\n",
        "            'f_adiff_mean': float(adiff_mean),\n",
        "            **ctx\n",
        "        }\n",
        "        if label is not None:\n",
        "            row['label_rule'] = label\n",
        "\n",
        "        out_rows.append(row)\n",
        "        cur += pd.to_timedelta(step)\n",
        "\n",
        "    feats = pd.DataFrame(out_rows).sort_values('t_center').reset_index(drop=True)\n",
        "    return feats\n",
        "\n",
        "feats = make_rolling_features(sub[['timestamp','value','traffic_load_proxy','rule_threshold','anomaly']].copy(),\n",
        "                              window='30s', step='15s')\n",
        "print(feats.shape)\n",
        "feats.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80c20362",
      "metadata": {
        "id": "80c20362"
      },
      "source": [
        "## 5) Temporal split and scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "746cc5ff",
      "metadata": {
        "id": "746cc5ff"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Temporal split (70/30)\n",
        "cut = int(len(feats)*0.7)\n",
        "train = feats.iloc[:cut].copy()\n",
        "test  = feats.iloc[cut:].copy()\n",
        "\n",
        "feature_cols = [c for c in feats.columns if c.startswith('f_') or c.startswith('ctx_')]\n",
        "label_col = 'label_rule' if 'label_rule' in feats.columns else None\n",
        "\n",
        "scaler = RobustScaler()\n",
        "Xs_tr = scaler.fit_transform(train[feature_cols])\n",
        "Xs_te = scaler.transform(test[feature_cols])\n",
        "\n",
        "# Save scaler\n",
        "import joblib\n",
        "joblib.dump(scaler, os.path.join(ARTIFACT_DIR, \"scaler.joblib\"))\n",
        "print(\"Saved:\", os.path.join(ARTIFACT_DIR, \"scaler.joblib\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20ae5376",
      "metadata": {
        "id": "20ae5376"
      },
      "source": [
        "## 6) Unsupervised model: Isolation Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ee5d1d",
      "metadata": {
        "id": "d0ee5d1d"
      },
      "outputs": [],
      "source": [
        "\n",
        "iso = IsolationForest(\n",
        "    n_estimators=300,\n",
        "    contamination='auto',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "iso.fit(Xs_tr)\n",
        "\n",
        "# Scores: higher -> more normal in sklearn's decision_function\n",
        "score_tr = iso.decision_function(Xs_tr)\n",
        "score_te = iso.decision_function(Xs_te)\n",
        "\n",
        "# Save model\n",
        "import joblib, json\n",
        "joblib.dump(iso, os.path.join(ARTIFACT_DIR, \"isoforest.pkl\"))\n",
        "print(\"Saved:\", os.path.join(ARTIFACT_DIR, \"isoforest.pkl\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a501520",
      "metadata": {
        "id": "3a501520"
      },
      "source": [
        "## 7) Threshold calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0bcd547",
      "metadata": {
        "id": "a0bcd547"
      },
      "outputs": [],
      "source": [
        "\n",
        "def pick_threshold(scores, quantile=0.005):\n",
        "    # Convert to alerts for the lowest quantile of normality score\n",
        "    thr = np.quantile(scores, quantile)\n",
        "    return float(thr)\n",
        "\n",
        "thr = pick_threshold(score_tr, quantile=0.005)  # ~0.5% most abnormal on train become alerts\n",
        "print(\"Chosen threshold:\", thr)\n",
        "\n",
        "with open(os.path.join(ARTIFACT_DIR, \"threshold.json\"), \"w\") as f:\n",
        "    json.dump({\"score_threshold\": thr}, f)\n",
        "print(\"Saved:\", os.path.join(ARTIFACT_DIR, \"threshold.json\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f72454a",
      "metadata": {
        "id": "1f72454a"
      },
      "source": [
        "## 8) Evaluation (if weak labels exist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb8e3f64",
      "metadata": {
        "id": "bb8e3f64"
      },
      "outputs": [],
      "source": [
        "\n",
        "alerts_te = (score_te < thr).astype(int)\n",
        "\n",
        "if label_col is not None and label_col in test.columns:\n",
        "    y_true = test[label_col].values\n",
        "    # PR metrics (preferred for rare events)\n",
        "    # For AP, we need a continuous score where higher means more likely positive.\n",
        "    # We invert the normality score to an anomaly score:\n",
        "    anomaly_score_te = -score_te\n",
        "    ap = average_precision_score(y_true, anomaly_score_te)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_true, alerts_te, average='binary', zero_division=0)\n",
        "    print(f\"AP: {ap:.3f}  Precision: {p:.3f}  Recall: {r:.3f}  F1: {f1:.3f}\")\n",
        "else:\n",
        "    print(\"No labels available; showing score distribution only.\")\n",
        "\n",
        "# Plot score timeline + threshold\n",
        "fig = plt.figure()\n",
        "plt.plot(test['t_center'], score_te, label='score')\n",
        "plt.axhline(thr, linestyle='--')\n",
        "plt.title(\"IsolationForest score (higher=more normal)\")\n",
        "plt.xlabel(\"time\"); plt.ylabel(\"score\")\n",
        "plt.show()\n",
        "\n",
        "# Alert markers\n",
        "fig = plt.figure()\n",
        "plt.plot(test['t_center'], alerts_te, drawstyle='steps-post')\n",
        "plt.title(\"Alerts (test)\")\n",
        "plt.xlabel(\"time\"); plt.ylabel(\"alert\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "565fa692",
      "metadata": {
        "id": "565fa692"
      },
      "source": [
        "## 9) Mini scoring loop (stream-friendly illustration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0303d87",
      "metadata": {
        "id": "f0303d87"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Pseudostream: iterate windows over the test set and emit events\n",
        "events = []\n",
        "for t, s, a in zip(test['t_center'], score_te, (score_te < thr).astype(int)):\n",
        "    if a == 1:\n",
        "        events.append({\"t\": str(t), \"event\": \"ALERT\", \"score\": float(s)})\n",
        "len(events), events[:5]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54f1c422",
      "metadata": {
        "id": "54f1c422"
      },
      "source": [
        "## 10) Next steps\n",
        "\n",
        "- Expand features (FFT band powers for accel/strain).\n",
        "- Per-sensor models vs. global per-type models.\n",
        "- Cost-aware thresholding (target ≤ 1 false alarm/day).\n",
        "- Add drift monitoring (score median + MAD over time).\n",
        "- Integrate with Streamlit dashboard (plot raw signals + score + alerts).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}